{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Decision trees\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "#import graphviz\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the data that we want to create a DT on.\n",
    "# import the fx data , econ and value data for EURUSD.\n",
    "# then create the features (on trend and econ data) standardise and run a DT on the x_train sample.\n",
    "# what is  target? 1 day ahead or long days ahead? trade on binary data.\n",
    "csv_file = {\"FXData\" : r\"C:\\Users\\edgil\\Documents\\Masters\\dissertation\\data\\CurrencyData.csv\",\n",
    "            \"ValueData\" : r\"\",\n",
    "            \"EconData\" : r\"\",\n",
    "            }\n",
    "fxdata = pd.read_csv(csv_file[\"FXData\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this to make sure the dates are imported correctly as UK dates\n",
    "fxdata['Date'] = pd.to_datetime(fxdata['Date'], format= '%d/%m/%Y %H:%M')\n",
    "# Separate out the EURUSD factor\n",
    "eurusd = fxdata[[\"Date\", \"EURUSD\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edgil\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  \nC:\\Users\\edgil\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  # This is added back by InteractiveShellApp.init_path()\nC:\\Users\\edgil\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  if sys.path[0] == '':\nC:\\Users\\edgil\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:13: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  del sys.path[0]\nC:\\Users\\edgil\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:14: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  \nC:\\Users\\edgil\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:15: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# Build out the featur set on price, this may need to be created using funcional process.\n",
    "eurusd[\"logret\"] = np.log(eurusd[\"EURUSD\"]) - np.log(eurusd[\"EURUSD\"].shift(1))\n",
    "# Standardising the daily rets and accumulating the standardised returns, or should we sum the % ret and standardise by its own history\n",
    "# is difference between different accumulated retusn horizons the same as the macd?\n",
    "# should we standardise by the 1 year forward vol?\n",
    "targetlkbk = 5\n",
    "lkbk1M = 22\n",
    "lkbk3M = 66\n",
    "lkbk6M = 132\n",
    "lkbk9M = 198\n",
    "eurusd['1MRet'] = eurusd[\"logret\"].rolling(lkbk1M).sum()\n",
    "eurusd['3MRet'] = eurusd[\"logret\"].rolling(lkbk3M).sum()\n",
    "eurusd['6MRet'] = eurusd[\"logret\"].rolling(lkbk6M).sum()\n",
    "eurusd['9MRet'] = eurusd[\"logret\"].rolling(lkbk9M).sum()\n",
    "eurusd['1Mv3MRet'] = eurusd[\"1MRet\"].sub(eurusd[\"3MRet\"])\n",
    "eurusd['1Mv6MRet'] = eurusd[\"1MRet\"].sub(eurusd[\"6MRet\"])\n",
    "# create a target vector to train on.\n",
    "# must think deeply about what this will look like\n",
    "# to start, this is a 1M forard return calculation, is it right to use overlapping 1M fwd rets? it seems not...\n",
    "eurusd[\"target\"] = eurusd['logret'].iloc[::-1].rolling(targetlkbk).sum().values[::-1]\n",
    "eurusd['target_binary'] = eurusd['target'].apply(np.sign) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very important step is to truncate the data so that we do not see the last 1 year of data.\n",
    "# Q. should we not have a rollign window type of model? or just always aggregate the data from the start?\n",
    "# how ong is testing? we should we the train and test to sizes which make sense to the type of model we use going forward.\n",
    "eurusd = eurusd.loc[eurusd['Date'] < \"2018-01-01 00:00\"]\n",
    "# train size should be at least 5 years?\n",
    "eurusd_train = eurusd.loc[eurusd['Date'] < \"2003-01-01 00:00\"]\n",
    "eurusd_test = eurusd.loc[eurusd['Date'] > \"2010-02-01 00:00\"]\n",
    "# create a target vector to train on.\n",
    "# must think deeply about what this will look like\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18768, 8)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standardise the data using sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# standardise the data now.\n",
    "# remove dates\n",
    "data_cols = [col for col in eurusd_train.columns if col not in [\"Date\", \"target\", \"target_binary\"]]\n",
    "data_train = eurusd_train.loc[:, data_cols]\n",
    "data_test = eurusd_test.loc[:, data_cols]\n",
    "eurusd_train_normed = pd.DataFrame(scaler.fit_transform(data_train), columns = list(data_train.columns))\n",
    "eurusd_test_normed = pd.DataFrame(scaler.transform(data_test), columns = list(data_test.columns)) \n",
    "#print(eurusd_train_normed.tail(50))\n",
    "eurusd_train_normed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### This is where we choose and set up the models!\n",
    "# need to re do the models here.\n",
    "X = eurusd_train_normed[['1Mv3MRet', \"1Mv6MRet\"]]\n",
    "Y = eurusd_train[\"target_binary\"]\n",
    "X_test = eurusd_test_normed\n",
    "Y_test = eurusd_test[\"target_binary\"]\n",
    "# clean the data and nan values\n",
    "X = X.replace(np.nan, 0)\n",
    "Y = Y.replace(np.nan, 0)\n",
    "X_test = X_test.replace(np.nan, 0)\n",
    "Y_test = Y_test.replace(np.nan, 0)\n",
    "#RF = RandomForestClassifier(n_estimators = 150, max_features = 5)\n",
    "#RF.fit(X, Y)\n",
    "clf = tree.DecisionTreeClassifier(max_leaf_nodes = 6, max_depth = 8)\n",
    "clf = clf.fit(X, Y)\n",
    "#tree.export_graphviz(clf, out_file='tree.dot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualise the data\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "\n",
    "dot_data = StringIO()\n",
    "\"\"\"export_graphviz(clf, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True)\n",
    "                \"\"\"\n",
    "tree.export_graphviz(clf, out_file=dot_data, class_names=['Sell',\"Hold\",\"Buy\"]\n",
    "                     , filled=True, rounded=True, special_characters = True) \n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue()) \n",
    "graph.write_pdf(\"treey.pdf\") \n",
    "#graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "#Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'round-table.gv.pdf'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rough work\n",
    "#eurusd = eurusd.replace(np.nan, 0)\n",
    "#print(eurusd.tail(25))\n",
    "#eurusd[\"target\"] = eurusd['logret'].iloc[::-1].rolling(2).sum().values[::-1]\n",
    "#print(eurusd.head(10))\n",
    "#data_cols = [col for col in eurusd_train.columns if col not in [\"Date\", \"target\", \"target_binary\"]]\n",
    "#print(data_cols)\n",
    "from graphviz import Digraph\n",
    "\n",
    "dot = Digraph(comment=\"The round table\")\n",
    "dot.node('A', 'King Arthur')\n",
    "dot.node('B', 'Sir Bedevere the Wise')\n",
    "dot.node('L', 'Sir Lancelot the Brave')\n",
    "dot.render('round-table.gv', view=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edgil\\Documents\\Masters\\dissertation\\code\\model_functions.py:65: RuntimeWarning: divide by zero encountered in true_divide\n  return data_df['logret'].iloc[::-1].shift(2).rolling(trade_horizon).sum().values[::-1]/data_df['logret'].iloc[::-1].shift(2).rolling(trade_horizon).std().values[::-1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_row 25000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ML model now end date : 2006-10-16 19:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.4s finished\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_row 50000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ML model now end date : 2007-12-27 07:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.4s finished\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_row 75000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ML model now end date : 2011-12-23 23:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.5s finished\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_row 100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ML model now end date : 2015-12-22 15:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.5s finished\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from model_functions import *\n",
    "from run_decision_tree import *\n",
    "import datetime\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "file_location = r\"C:\\Users\\edgil\\Documents\\Masters\\dissertation\\code64\\data_set\\CcyData.csv\"\n",
    "performance_store = {\"data_size\" : [], \"Accuracy_Score\" : [], \"ntree\": [],\n",
    "                     \"Info_Ratio\" : [], \"run_time\" : [], \"train_date_st\": [], \"test_date_st\": []}\n",
    "########################### Set Model Paramaters #############################\n",
    "# this looks back over a set period as the memory for the LSTM\n",
    "ntree = 3 # [21, 66]\n",
    "max_features = 5\n",
    "test_buffer = 5\n",
    "data_size = 25000\n",
    "test_split = 0.75\n",
    "# signal threshold, when using classifier\n",
    "thold = 0.55\n",
    "total_data_needed = int(data_size*(1 + test_split)) + test_buffer\n",
    "# standardisation window\n",
    "window = 17500\n",
    "###### Set Targets ##############\n",
    "trade_horizon = 24 # in hours\n",
    "use_risk_adjusted = True # if True: training on the sharpe return else raw\n",
    "use_binary = False # set to true if you are using the risk adjusted and want it binary for classification score\n",
    "use_classifier = False\n",
    "################### Standardise Entire Dataset using rolling lookback windows ###############\n",
    "data_normed, model_features = initialise_process(file_location, trade_horizon, window, use_risk_adjusted)\n",
    "#data_normed = data_normed.replace(np.nan, 0)\n",
    "start_row = 0\n",
    "use_separated_chunk = False # Use a rolling window to train and test\n",
    "################ Loop through the full dataset in terms of the training and testing.\n",
    "while start_row < data_normed.shape[0]:\n",
    "    # first check if there is enough data left\n",
    "    if (start_row + total_data_needed) > data_normed.shape[0]:\n",
    "        # if we are about to go over the limit, then just return the last data_size + test size proportion of data\n",
    "        trunc_data = data_normed.iloc[-total_data_needed:,:]\n",
    "    # we need to increment over the data size\n",
    "    if use_separated_chunk:\n",
    "        # this means we jump across the full previous train and test data\n",
    "        trunc_data = data_normed.loc[start_row:,:]\n",
    "        start_row += total_data_needed\n",
    "    else:\n",
    "        # this rools the data so that the new training will overlap on the old test set and create a new separated test set\n",
    "        trunc_data = data_normed.loc[start_row:,:]\n",
    "        start_row += data_size\n",
    "    # standardise the data\n",
    "    #################### Set up training and testing ########################\n",
    "    print(\"start_row %s\" % start_row)\n",
    "    trunc_data.to_csv(r\"C:\\Users\\edgil\\Documents\\Masters\\dissertation\\code\\dataset\\TruncData_%s.csv\" % str(start_row)) \n",
    "    # create data_set\n",
    "    train , test = create_train_test_file(trunc_data, data_size, test_split, test_buffer)\n",
    "    train_sample = train[model_features]\n",
    "    test_sample = test[model_features]\n",
    "    # train the model\n",
    "    # verbose = 1 gives the output of the training.\n",
    "    print(\"Starting ML model now end date : %s\" % train['Date'].iloc[-1] )\n",
    "    start_time = datetime.datetime.now()\n",
    "    results, acc_score = decision_tree(train_sample, test_sample,use_classifier, use_risk_adjusted,ntree, max_features)\n",
    "    run_time = datetime.datetime.now() - start_time\n",
    "    test_results = backtester(results, test, trade_horizon)[0]\n",
    "    strat_return = backtester(results, test, trade_horizon)[1]\n",
    "    information_ratio = backtester(results, test, trade_horizon)[2]\n",
    "    train_date = train['Date'].iloc[0]\n",
    "    test_date = test['Date'].iloc[0]\n",
    "    performance_df = update_performance(data_size, ntree, acc_score , information_ratio, \n",
    "                                        run_time, train_date, test_date, performance_store)\n",
    "    save_results = r\"C:\\Users\\edgil\\Documents\\Masters\\dissertation\\code\\dataset\\test_result_start%s_useriskadj%s_ntree%s_thold%s.csv\" % (start_row, \n",
    "                    use_risk_adjusted, ntree, thold)\n",
    "    save_perf_df = r\"C:\\Users\\edgil\\Documents\\Masters\\dissertation\\code\\dataset\\performance_df_ntree%s_st_row%s.csv\" % (ntree,start_row) \n",
    "    test_results.to_csv(save_results,index = False)\n",
    "    performance_df.to_csv(save_perf_df,index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edgil\\Documents\\Masters\\dissertation\\code\\model_functions.py:77: RuntimeWarning: divide by zero encountered in true_divide\n  return data_df['logret'].iloc[::-1].shift(2).rolling(trade_horizon).sum().values[::-1]/data_df['logret'].iloc[::-1].shift(2).rolling(trade_horizon).std().values[::-1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_row 1500\nStarting ML model now end date : 2003-01-14 16:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 1500\nstart_row 3000\nStarting ML model now end date : 2003-04-11 04:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 3000\nstart_row 4500\nStarting ML model now end date : 2003-07-08 16:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 4500\nstart_row 6000\nStarting ML model now end date : 2003-10-03 04:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 6000\nstart_row 7500\nStarting ML model now end date : 2003-12-30 16:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 7500\nstart_row 9000\nStarting ML model now end date : 2004-03-26 04:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 9000\nstart_row 10500\nStarting ML model now end date : 2004-06-22 16:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 10500\nstart_row 12000\nStarting ML model now end date : 2004-09-17 04:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 12000\nstart_row 13500\nStarting ML model now end date : 2004-12-14 16:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 13500\nstart_row 15000\nStarting ML model now end date : 2005-03-11 04:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 15000\nstart_row 16500\nStarting ML model now end date : 2005-06-07 16:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 16500\nstart_row 18000\nStarting ML model now end date : 2005-09-02 04:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 18000\nstart_row 19500\nStarting ML model now end date : 2005-11-29 16:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 19500\nstart_row 21000\nStarting ML model now end date : 2006-02-24 04:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 21000\nstart_row 22500\nStarting ML model now end date : 2006-05-23 16:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 22500\nstart_row 24000\nStarting ML model now end date : 2006-08-18 04:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 24000\nstart_row 25500\nStarting ML model now end date : 2006-11-14 16:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 25500\nstart_row 27000\nStarting ML model now end date : 2007-02-09 04:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 27000\nstart_row 28500\nStarting ML model now end date : 2007-05-08 16:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 28500\nstart_row 30000\nStarting ML model now end date : 2007-08-03 04:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 30000\nstart_row 31500\nStarting ML model now end date : 2007-10-30 16:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 31500\nstart_row 33000\nStarting ML model now end date : 2008-01-25 04:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 33000\nstart_row 34500\nStarting ML model now end date : 2008-04-22 16:00:00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3c7fd4f3ec5b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mntree\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mntrees\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         results, acc_score = decision_tree(train_sample, test_sample,use_classifier,\n\u001b[1;32m---> 74\u001b[1;33m                                            use_risk_adjusted,ntree, max_features, max_depth)\n\u001b[0m\u001b[0;32m     75\u001b[0m         \u001b[0mrun_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0mtest_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbacktester\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrade_horizon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Masters\\dissertation\\code\\run_decision_tree.py\u001b[0m in \u001b[0;36mdecision_tree\u001b[1;34m(train, test, use_classifier, use_risk_adjusted, ntree, max_features, max_depth)\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;31m# ass in code for regresion classifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mRF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mntree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_depth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0mRF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m     \u001b[1;31m# run training on the test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    331\u001b[0m                     \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[1;32m--> 333\u001b[1;33m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[0;32m    334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m             \u001b[1;31m# Collect newly grown trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    918\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 920\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    921\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    922\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    757\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[0;32m    117\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'balanced'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    799\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    800\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 801\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m    802\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    803\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    364\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 366\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "##### RUN LOOP on NTREE ####\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from model_functions import *\n",
    "from run_decision_tree import *\n",
    "import datetime\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "file_location = r\"C:\\Users\\edgil\\Documents\\Masters\\dissertation\\code64\\data_set\\CcyData.csv\"\n",
    "performance_store = {\"data_size\" : [], \"Accuracy_Score\" : [], \"ntree\": [],\n",
    "                     \"Info_Ratio\" : [], \"run_time\" : [], \"train_date_st\": [], \"test_date_st\": []}\n",
    "########################### Set Model Paramaters #############################\n",
    "# this looks back over a set period as the memory for the LSTM\n",
    "ntrees = [150] # [i for i in range(25,301,25)] # [21, 66]\n",
    "max_features = 5\n",
    "test_buffer = 5\n",
    "max_depth = 30\n",
    "data_size = 1500 # train every 3 months\n",
    "# I.e. append the data into one df over each training window, but also use all available up until that point\n",
    "concat_results = True \n",
    "# if the number is > 1, then the code takes that as the number of test points you want to use\n",
    "test_split = 460 # roughly one month test ahead\n",
    "# signal threshold, when using classifier\n",
    "thold = 0.55\n",
    "total_data_needed = get_total_data_needed(test_split,data_size,test_buffer)\n",
    "# standardisation window\n",
    "window = 17500\n",
    "###### Set Targets ##############\n",
    "trade_horizon = 24 # in hours\n",
    "use_risk_adjusted = True # if True: training on the sharpe return else raw\n",
    "use_binary = False # set to true if you are using the risk adjusted and want it binary for classification score\n",
    "use_classifier = True\n",
    "################### Standardise Entire Dataset using rolling lookback windows ###############\n",
    "data_normed, model_features = initialise_process(file_location, trade_horizon, window, use_risk_adjusted)\n",
    "#data_normed = data_normed.replace(np.nan, 0)\n",
    "start_row = 0\n",
    "use_separated_chunk = False # Use a rolling window to train and test\n",
    "################ Loop through the full dataset in terms of the training and testing.\n",
    "while start_row < data_normed.shape[0]:\n",
    "    # first check if there is enough data left\n",
    "    if (start_row + total_data_needed) > data_normed.shape[0]:\n",
    "        # if we are about to go over the limit, then just return the last data_size + test size proportion of data\n",
    "        trunc_data = data_normed.iloc[-total_data_needed:,:]\n",
    "    # we need to increment over the data size\n",
    "    if use_separated_chunk:\n",
    "        # this means we jump across the full previous train and test data\n",
    "        trunc_data = data_normed.loc[start_row:,:]\n",
    "        start_row += total_data_needed\n",
    "    if concat_results:\n",
    "        # in this instance, we can to add to the start row first before chunking the data\n",
    "        start_row += data_size\n",
    "        # we are training on all data available up until that point, and testing x timeperiods ahead\n",
    "        trunc_data = data_normed.loc[:start_row,:]  \n",
    "    else:\n",
    "        # this rolls the data so that the new training will overlap on the old test set and create a new separated test set\n",
    "        trunc_data = data_normed.loc[start_row:,:]\n",
    "        start_row += data_size\n",
    "    # standardise the data\n",
    "    #################### Set up training and testing ########################\n",
    "    print(\"start_row %s\" % start_row)\n",
    "    #trunc_data.to_csv(r\"C:\\Users\\edgil\\Documents\\Masters\\dissertation\\code\\dataset\\TruncData_%s.csv\" % str(start_row)) \n",
    "    # create data_set\n",
    "    train , test = create_train_test_file(trunc_data, data_size, test_split, test_buffer, concat_results)\n",
    "    train_sample = train[model_features]\n",
    "    test_sample = test[model_features]\n",
    "    # train the model\n",
    "    # verbose = 1 gives the output of the training.\n",
    "    print(\"Starting ML model now end date : %s\" % train['Date'].iloc[-1] )\n",
    "    start_time = datetime.datetime.now()\n",
    "    for ntree in ntrees:\n",
    "        results, acc_score = decision_tree(train_sample, test_sample,use_classifier,\n",
    "                                           use_risk_adjusted,ntree, max_features, max_depth)\n",
    "        run_time = datetime.datetime.now() - start_time\n",
    "        test_results = backtester(results, test, trade_horizon)[0]\n",
    "        strat_return = backtester(results, test, trade_horizon)[1]\n",
    "        information_ratio = backtester(results, test, trade_horizon)[2]\n",
    "        train_date = train['Date'].iloc[0]\n",
    "        test_date = test['Date'].iloc[0]\n",
    "        performance_df = update_performance(data_size, ntree, acc_score , information_ratio, \n",
    "                                            run_time, train_date, test_date, performance_store)\n",
    "    save_test_df = r\"C:\\Users\\edgil\\Documents\\Masters\\dissertation\\code\\dataset\\testresults_df_ntree%s_st_row%s_use_risk%s_use_SepChunk%s_concat%s.csv\" % (ntree,start_row,use_risk_adjusted,use_separated_chunk,concat_results) \n",
    "    if concat_results:\n",
    "        # i.e. on the first iteration, keep reulsts as is, then we appedn at subsequent iterations\n",
    "        if start_row == data_size:\n",
    "            master_df = test_results\n",
    "        else:\n",
    "            master_df = pd.merge(master_df, test_results, how=\"left\", on=\"Date\").fillna(0)\n",
    "    else:\n",
    "        test_results.to_csv(save_test_df, index = False)\n",
    "    print(\"finished with row: %s\" % str(start_row))\n",
    "    \n",
    "save_perf_df = r\"C:\\Users\\edgil\\Documents\\Masters\\dissertation\\code\\dataset\\performance_df_ntree%s_st_row%s.csv\" % (ntree,start_row) \n",
    "performance_df.to_csv(save_perf_df,index = False)\n",
    "if concat_results:\n",
    "    master_df.to_csv(save_test_df, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from model_functions import *\n",
    "from run_decision_tree import *\n",
    "import datetime\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "file_location = r\"C:\\Users\\edgil\\Documents\\Masters\\dissertation\\code64\\data_set\\CcyData.csv\"\n",
    "performance_store = {\"data_size\" : [], \"Accuracy_Score\" : [], \"ntree\": [],\n",
    "                     \"Info_Ratio\" : [], \"run_time\" : [], \"train_date_st\": [], \"test_date_st\": []}\n",
    "########################### Set Model Paramaters #############################\n",
    "# this looks back over a set period as the memory for the LSTM\n",
    "ntrees = [150] # [i for i in range(25,301,25)] # [21, 66]\n",
    "max_features = 5\n",
    "test_buffer = 5\n",
    "max_depth = 30\n",
    "data_size = 1500 # train every 3 months\n",
    "# I.e. append the data into one df over each training window, but also use all available up until that point\n",
    "concat_results = True \n",
    "# if the number is > 1, then the code takes that as the number of test points you want to use\n",
    "test_split = 460 # roughly one month test ahead\n",
    "# signal threshold, when using classifier\n",
    "thold = 0.55\n",
    "total_data_needed = get_total_data_needed(test_split,data_size,test_buffer)\n",
    "# standardisation window\n",
    "window = 17500\n",
    "###### Set Targets ##############\n",
    "trade_horizon = 24 # in hours\n",
    "use_risk_adjusted = True # if True: training on the sharpe return else raw\n",
    "use_binary = False # set to true if you are using the risk adjusted and want it binary for classification score\n",
    "use_classifier = True\n",
    "################### Standardise Entire Dataset using rolling lookback windows ###############\n",
    "data_normed, model_features = initialise_process(file_location, trade_horizon, window, use_risk_adjusted)\n",
    "#data_normed = data_normed.replace(np.nan, 0)\n",
    "start_row = 0\n",
    "use_separated_chunk = False # Use a rolling window to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        spot_v_HF  spot_v_MF  spot_v_LF  HF_ema_diff  MF_ema_diff  \\\n17500   -1.027209   0.195502   1.015046    -1.152828     1.191202   \n17501   -0.919513   0.217672   1.028499    -1.132625     1.188294   \n17502   -0.904531   0.218718   1.028353    -1.114026     1.185407   \n17503   -1.024612   0.186997   1.007060    -1.110138     1.182369   \n17504   -1.001681   0.188123   1.006926    -1.105036     1.179353   \n17505   -0.912394   0.205618   1.017365    -1.092261     1.176446   \n17506   -0.895699   0.206675   1.017223    -1.079858     1.173560   \n17507   -0.763735   0.235801   1.035203    -1.056332     1.170842   \n17508   -0.754912   0.236745   1.035041    -1.035121     1.168144   \n17509   -0.745661   0.237683   1.034878    -1.015855     1.165466   \n17510   -0.736127   0.238608   1.034713    -0.998226     1.162807   \n17511   -0.880646   0.202090   1.010386    -0.997241     1.159970   \n17512   -0.861522   0.203110   1.010235    -0.994727     1.157153   \n17513   -0.804861   0.213482   1.016124    -0.987140     1.154406   \n17514   -0.790000   0.214459   1.015969    -0.979195     1.151679   \n17515   -0.794963   0.210750   1.012795    -0.972877     1.148947   \n17516   -0.779902   0.211745   1.012657    -0.965995     1.146235   \n17517   -0.678666   0.233790   1.026107    -0.950078     1.143653   \n17518   -0.669964   0.234714   1.025966    -0.935313     1.141090   \n17519   -0.661133   0.235630   1.025824    -0.921526     1.138545   \n17520   -0.652243   0.236562   1.025702    -0.908561     1.136018   \n17521   -0.643308   0.237485   1.025579    -0.896299     1.133510   \n17522   -0.634359   0.238392   1.025447    -0.884639     1.131019   \n17523   -0.808739   0.194832   0.996632    -0.891616     1.128313   \n17524   -0.788852   0.195855   0.996516    -0.895835     1.125626   \n17525   -0.374805   0.292804   1.058290    -0.858673     1.123463   \n17526   -0.381318   0.293515   1.058128    -0.826534     1.121315   \n17527   -0.627232   0.235722   1.020229    -0.822434     1.118874   \n17528   -0.615914   0.236601   1.020094    -0.817630     1.116450   \n17529    0.031759   0.391906   1.119576    -0.749313     1.114856   \n...           ...        ...        ...          ...          ...   \n112650   1.003231   1.094885   1.641643     0.884779     1.001328   \n112651   1.068507   1.115936   1.652688     0.910203     1.002886   \n112652   0.934316   1.092002   1.641212     0.919710     1.004338   \n112653   0.651987   1.032163   1.611734     0.900271     1.005534   \n112654   0.700968   1.044448   1.618312     0.888559     1.006775   \n112655   0.598963   1.022968   1.608002     0.868434     1.007921   \n112656   0.610608   1.026301   1.610087     0.852260     1.009074   \n112657   0.709194   1.049892   1.622303     0.848263     1.010318   \n112658   0.634138   1.035109   1.615352     0.837520     1.011494   \n112659   0.740345   1.060966   1.628699     0.839026     1.012771   \n112660   0.623418   1.037126   1.617237     0.828879     1.013942   \n112661   0.631506   1.040426   1.619308     0.820999     1.015121   \n112662   0.717377   1.061772   1.630387     0.822916     1.016381   \n112663   0.817452   1.087605   1.643725     0.834904     1.017741   \n112664   0.754106   1.077191   1.638996     0.839336     1.019050   \n112665   1.019729   1.141506   1.671533     0.870229     1.020617   \n112666   1.249826   1.201141   1.701789     0.920936     1.022420   \n112667   1.201425   1.199474   1.701536     0.961148     1.024207   \n112668   1.275839   1.225013   1.714837     1.004531     1.026089   \n112669   1.386553   1.259568   1.732650     1.054429     1.028102   \n112670   1.177125   1.221438   1.714289     1.077870     1.029949   \n112671   1.257689   1.246969   1.727591     1.107223     1.031890   \n112672   1.550782   1.322426   1.765764     1.163251     1.034130   \n112673   1.728741   1.374971   1.792600     1.231273     1.036573   \n112674   1.661641   1.372772   1.792246     1.285231     1.038995   \n112675   1.443506   1.334234   1.773812     1.311607     1.041245   \n112676   1.309290   1.311706   1.763301     1.322125     1.043392   \n112677   1.046234   1.257320   1.736928     1.305618     1.045304   \n112678   1.133976   1.280526   1.749067     1.300626     1.047301   \n112679   1.107476   1.278628   1.748751     1.294216     1.049280   \n\n        LF_ema_diff  LDN   NY  Asia    target                 Date      CCY  \\\n17500      2.470113  0.0  0.0     1  2.004172  2002-10-18 04:00:00  0.97185   \n17501      2.468936  0.0  0.0     1  2.004172  2002-10-18 05:00:00  0.97230   \n17502      2.467761  0.0  0.0     1  0.931286  2002-10-18 06:00:00  0.97230   \n17503      2.466550  1.0  0.0     0  0.931286  2002-10-18 07:00:00  0.97160   \n17504      2.465341  1.0  0.0     0  3.921735  2002-10-18 08:00:00  0.97160   \n17505      2.464151  1.0  0.0     0  3.921735  2002-10-18 09:00:00  0.97195   \n17506      2.462964  1.0  0.0     0  2.436377  2002-10-18 10:00:00  0.97195   \n17507      2.461809  1.0  0.0     0  2.436377  2002-10-18 11:00:00  0.97255   \n17508      2.460656  1.0  0.0     0  1.578233  2002-10-18 12:00:00  0.97255   \n17509      2.459504  0.5  0.5     0  1.578233  2002-10-18 13:00:00  0.97255   \n17510      2.458354  0.5  0.5     0  2.043298  2002-10-18 14:00:00  0.97255   \n17511      2.457163  0.5  0.5     0  2.043298  2002-10-18 15:00:00  0.97175   \n17512      2.455974  0.5  0.5     0  2.097228  2002-10-18 16:00:00  0.97175   \n17513      2.454796  0.5  0.5     0  2.097228  2002-10-18 17:00:00  0.97195   \n17514      2.453621  0.0  1.0     0  2.537829  2002-10-18 18:00:00  0.97195   \n17515      2.452441  0.0  1.0     0  2.537829  2002-10-18 19:00:00  0.97185   \n17516      2.451263  0.0  1.0     0  1.254175  2002-10-18 20:00:00  0.97185   \n17517      2.450111  0.0  1.0     0  1.254175  2002-10-18 21:00:00  0.97230   \n17518      2.448959  0.0  1.0     0  1.254175  2002-10-18 22:00:00  0.97230   \n17519      2.447809  0.0  0.0     1  1.254175  2002-10-18 23:00:00  0.97230   \n17520      2.446661  0.0  0.0     1  1.034628  2002-10-21 00:00:00  0.97230   \n17521      2.445514  0.0  0.0     1  1.034628  2002-10-21 01:00:00  0.97230   \n17522      2.444369  0.0  0.0     1  1.784786  2002-10-21 02:00:00  0.97230   \n17523      2.443175  0.0  0.0     1  1.784786  2002-10-21 03:00:00  0.97135   \n17524      2.441983  0.0  0.0     1  0.492866  2002-10-21 04:00:00  0.97135   \n17525      2.440900  0.0  0.0     1  0.492866  2002-10-21 05:00:00  0.97340   \n17526      2.439818  0.0  0.0     1 -0.232889  2002-10-21 06:00:00  0.97340   \n17527      2.438673  1.0  0.0     0 -0.232889  2002-10-21 07:00:00  0.97215   \n17528      2.437528  1.0  0.0     0  0.162795  2002-10-21 08:00:00  0.97215   \n17529      2.436558  1.0  0.0     0  0.162795  2002-10-21 09:00:00  0.97545   \n...             ...  ...  ...   ...       ...                  ...      ...   \n112650     2.007342  0.0  1.0     0  7.306080  2017-12-28 18:00:00  1.19530   \n112651     2.007420  0.0  1.0     0  7.394549  2017-12-28 19:00:00  1.19580   \n112652     2.007483  0.0  1.0     0  7.731257  2017-12-28 20:00:00  1.19530   \n112653     2.007511  0.0  1.0     0  8.034336  2017-12-28 21:00:00  1.19400   \n112654     2.007546  0.0  1.0     0  8.782839  2017-12-28 22:00:00  1.19430   \n112655     2.007568  0.0  0.0     1       NaN  2017-12-28 23:00:00  1.19385   \n112656     2.007593  0.0  0.0     1       NaN  2017-12-29 00:00:00  1.19395   \n112657     2.007632  0.0  0.0     1       NaN  2017-12-29 01:00:00  1.19450   \n112658     2.007662  0.0  0.0     1       NaN  2017-12-29 02:00:00  1.19420   \n112659     2.007708  0.0  0.0     1       NaN  2017-12-29 03:00:00  1.19480   \n112660     2.007740  0.0  0.0     1       NaN  2017-12-29 04:00:00  1.19430   \n112661     2.007775  0.0  0.0     1       NaN  2017-12-29 05:00:00  1.19440   \n112662     2.007822  0.0  0.0     1       NaN  2017-12-29 06:00:00  1.19490   \n112663     2.007885  1.0  0.0     0       NaN  2017-12-29 07:00:00  1.19550   \n112664     2.007943  1.0  0.0     0       NaN  2017-12-29 08:00:00  1.19530   \n112665     2.008039  1.0  0.0     0       NaN  2017-12-29 09:00:00  1.19675   \n112666     2.008172  1.0  0.0     0       NaN  2017-12-29 10:00:00  1.19810   \n112667     2.008304  1.0  0.0     0       NaN  2017-12-29 11:00:00  1.19810   \n112668     2.008451  1.0  0.0     0       NaN  2017-12-29 12:00:00  1.19870   \n112669     2.008620  0.5  0.5     0       NaN  2017-12-29 13:00:00  1.19950   \n112670     2.008766  0.5  0.5     0       NaN  2017-12-29 14:00:00  1.19870   \n112671     2.008928  0.5  0.5     0       NaN  2017-12-29 15:00:00  1.19930   \n112672     2.009135  0.5  0.5     0       NaN  2017-12-29 16:00:00  1.20100   \n112673     2.009375  0.5  0.5     0       NaN  2017-12-29 17:00:00  1.20220   \n112674     2.009613  0.0  1.0     0       NaN  2017-12-29 18:00:00  1.20220   \n112675     2.009829  0.0  1.0     0       NaN  2017-12-29 19:00:00  1.20140   \n112676     2.010031  0.0  1.0     0       NaN  2017-12-29 20:00:00  1.20095   \n112677     2.010201  0.0  1.0     0       NaN  2017-12-29 21:00:00  1.19980   \n112678     2.010385  0.0  1.0     0       NaN  2017-12-29 22:00:00  1.20035   \n112679     2.010569  0.0  0.0     1       NaN  2017-12-29 23:00:00  1.20035   \n\n          logret  \n17500   0.000000  \n17501   0.000463  \n17502   0.000000  \n17503  -0.000720  \n17504   0.000000  \n17505   0.000360  \n17506   0.000000  \n17507   0.000617  \n17508   0.000000  \n17509   0.000000  \n17510   0.000000  \n17511  -0.000823  \n17512   0.000000  \n17513   0.000206  \n17514   0.000000  \n17515  -0.000103  \n17516   0.000000  \n17517   0.000463  \n17518   0.000000  \n17519   0.000000  \n17520   0.000000  \n17521   0.000000  \n17522   0.000000  \n17523  -0.000978  \n17524   0.000000  \n17525   0.002108  \n17526   0.000000  \n17527  -0.001285  \n17528   0.000000  \n17529   0.003389  \n...          ...  \n112650  0.000084  \n112651  0.000418  \n112652 -0.000418  \n112653 -0.001088  \n112654  0.000251  \n112655 -0.000377  \n112656  0.000084  \n112657  0.000461  \n112658 -0.000251  \n112659  0.000502  \n112660 -0.000419  \n112661  0.000084  \n112662  0.000419  \n112663  0.000502  \n112664 -0.000167  \n112665  0.001212  \n112666  0.001127  \n112667  0.000000  \n112668  0.000501  \n112669  0.000667  \n112670 -0.000667  \n112671  0.000500  \n112672  0.001416  \n112673  0.000999  \n112674  0.000000  \n112675 -0.000666  \n112676 -0.000375  \n112677 -0.000958  \n112678  0.000458  \n112679  0.000000  \n\n[95180 rows x 13 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7500"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_row += data_size\n",
    "print(data_normed.loc[:,:])\n",
    "start_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "# verbose = 1 gives the output of the training.\n",
    "print(\"Starting ML model now end date : %s\" % train['Date'].iloc[-1] )\n",
    "start_time = datetime.datetime.now()\n",
    "X = train_sample.iloc[:,:-1]\n",
    "Y = train_sample[\"target\"].apply(np.sign)\n",
    "X_test = test_sample.iloc[:,:-1]\n",
    "Y_test = test_sample[\"target\"].apply(np.sign)\n",
    "# clean the data and nan values\n",
    "X = X.replace(np.nan, 0)\n",
    "Y = Y.replace(np.nan, 0)\n",
    "X_test = X_test.replace(np.nan, 0)\n",
    "Y_test = Y_test.replace(np.nan, 0)\n",
    "RF = RandomForestClassifier(n_estimators = ntree, max_features = 5, verbose = 1)\n",
    "RF.fit(X, Y)\n",
    "clf = tree.DecisionTreeClassifier(max_leaf_nodes = 6, max_depth = 8)\n",
    "clf = clf.fit(X, Y)\n",
    "run_time = datetime.datetime.now() - start_time\n",
    "# run training on the test data\n",
    "results = RF.predict(X_test)\n",
    "# The % threshold needed to trigger a signal either way\n",
    "if use_binary and use_risk_adjusted:\n",
    "    thold = 0.55\n",
    "    results = [signal(i, thold) for i in list(results)]\n",
    "    acc_score = get_accuracy(Y_test, results)\n",
    "# This needs to change to handle the change in the target\n",
    "predictions = pd.DataFrame({\"Date\" : test['Date'],\"Predictions\": results})\n",
    "test_results = pd.merge(test, predictions,how= \"left\", on= \"Date\").fillna(0)\n",
    "# calculate the returns of the signal\n",
    "test_results[\"scaled_signal\"] = test_results['Predictions'].shift(2).rolling(trade_horizon).sum()/trade_horizon\n",
    "# no shift needed as we have already done that in previous step\n",
    "test_results['strat_returns'] = test_results['logret']*test_results['scaled_signal']\n",
    "test_results['strat_returns_sum'] = test_results['strat_returns'].cumsum()\n",
    "strat_return = test_results['strat_returns'].sum()\n",
    "information_ratio = (test_results['strat_returns'].mean()*260)/(test_results['strat_returns'].std()*np.sqrt(260))\n",
    "\n",
    "# Store the data as needed\n",
    "performance_store['data_size'].append(data_size)\n",
    "performance_store['ntree'].append(ntree)\n",
    "performance_store['Accuracy_Score'].append(acc_score)\n",
    "performance_store['Info_Ratio'].append(information_ratio)\n",
    "performance_store['run_time'].append(run_time)\n",
    "performance_store['train_date_st'].append(train['Date'].iloc[0])\n",
    "performance_store['test_date_st'].append(test_results['Date'].iloc[0])\n",
    "performance_df = pd.DataFrame(performance_store)\n",
    "save_results = r\"C:\\Users\\edgil\\Documents\\Masters\\dissertation\\code\\dataset\\test_result_start%s_lkbk%s_ntree%s_thold%s.csv\" % (start_row, ntree, ntree, thold)\n",
    "test_results.to_csv(save_results,index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
