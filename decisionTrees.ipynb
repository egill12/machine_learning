{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Decision trees\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "#import graphviz\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the data that we want to create a DT on.\n",
    "# import the fx data , econ and value data for EURUSD.\n",
    "# then create the features (on trend and econ data) standardise and run a DT on the x_train sample.\n",
    "# what is  target? 1 day ahead or long days ahead? trade on binary data.\n",
    "csv_file = {\"FXData\" : r\"C:\\Users\\edgil\\Documents\\Masters\\dissertation\\data\\CurrencyData.csv\",\n",
    "            \"ValueData\" : r\"\",\n",
    "            \"EconData\" : r\"\",\n",
    "            }\n",
    "fxdata = pd.read_csv(csv_file[\"FXData\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this to make sure the dates are imported correctly as UK dates\n",
    "fxdata['Date'] = pd.to_datetime(fxdata['Date'], format= '%d/%m/%Y %H:%M')\n",
    "# Separate out the EURUSD factor\n",
    "eurusd = fxdata[[\"Date\", \"EURUSD\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edgil\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  \nC:\\Users\\edgil\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  # This is added back by InteractiveShellApp.init_path()\nC:\\Users\\edgil\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  if sys.path[0] == '':\nC:\\Users\\edgil\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:13: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  del sys.path[0]\nC:\\Users\\edgil\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:14: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  \nC:\\Users\\edgil\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:15: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# Build out the featur set on price, this may need to be created using funcional process.\n",
    "eurusd[\"logret\"] = np.log(eurusd[\"EURUSD\"]) - np.log(eurusd[\"EURUSD\"].shift(1))\n",
    "# Standardising the daily rets and accumulating the standardised returns, or should we sum the % ret and standardise by its own history\n",
    "# is difference between different accumulated retusn horizons the same as the macd?\n",
    "# should we standardise by the 1 year forward vol?\n",
    "targetlkbk = 5\n",
    "lkbk1M = 22\n",
    "lkbk3M = 66\n",
    "lkbk6M = 132\n",
    "lkbk9M = 198\n",
    "eurusd['1MRet'] = eurusd[\"logret\"].rolling(lkbk1M).sum()\n",
    "eurusd['3MRet'] = eurusd[\"logret\"].rolling(lkbk3M).sum()\n",
    "eurusd['6MRet'] = eurusd[\"logret\"].rolling(lkbk6M).sum()\n",
    "eurusd['9MRet'] = eurusd[\"logret\"].rolling(lkbk9M).sum()\n",
    "eurusd['1Mv3MRet'] = eurusd[\"1MRet\"].sub(eurusd[\"3MRet\"])\n",
    "eurusd['1Mv6MRet'] = eurusd[\"1MRet\"].sub(eurusd[\"6MRet\"])\n",
    "# create a target vector to train on.\n",
    "# must think deeply about what this will look like\n",
    "# to start, this is a 1M forard return calculation, is it right to use overlapping 1M fwd rets? it seems not...\n",
    "eurusd[\"target\"] = eurusd['logret'].iloc[::-1].rolling(targetlkbk).sum().values[::-1]\n",
    "eurusd['target_binary'] = eurusd['target'].apply(np.sign) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very important step is to truncate the data so that we do not see the last 1 year of data.\n",
    "# Q. should we not have a rollign window type of model? or just always aggregate the data from the start?\n",
    "# how ong is testing? we should we the train and test to sizes which make sense to the type of model we use going forward.\n",
    "eurusd = eurusd.loc[eurusd['Date'] < \"2018-01-01 00:00\"]\n",
    "# train size should be at least 5 years?\n",
    "eurusd_train = eurusd.loc[eurusd['Date'] < \"2003-01-01 00:00\"]\n",
    "eurusd_test = eurusd.loc[eurusd['Date'] > \"2010-02-01 00:00\"]\n",
    "# create a target vector to train on.\n",
    "# must think deeply about what this will look like\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18768, 8)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standardise the data using sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# standardise the data now.\n",
    "# remove dates\n",
    "data_cols = [col for col in eurusd_train.columns if col not in [\"Date\", \"target\", \"target_binary\"]]\n",
    "data_train = eurusd_train.loc[:, data_cols]\n",
    "data_test = eurusd_test.loc[:, data_cols]\n",
    "eurusd_train_normed = pd.DataFrame(scaler.fit_transform(data_train), columns = list(data_train.columns))\n",
    "eurusd_test_normed = pd.DataFrame(scaler.transform(data_test), columns = list(data_test.columns)) \n",
    "#print(eurusd_train_normed.tail(50))\n",
    "eurusd_train_normed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### This is where we choose and set up the models!\n",
    "# need to re do the models here.\n",
    "X = eurusd_train_normed[['1Mv3MRet', \"1Mv6MRet\"]]\n",
    "Y = eurusd_train[\"target_binary\"]\n",
    "X_test = eurusd_test_normed\n",
    "Y_test = eurusd_test[\"target_binary\"]\n",
    "# clean the data and nan values\n",
    "X = X.replace(np.nan, 0)\n",
    "Y = Y.replace(np.nan, 0)\n",
    "X_test = X_test.replace(np.nan, 0)\n",
    "Y_test = Y_test.replace(np.nan, 0)\n",
    "#RF = RandomForestClassifier(n_estimators = 150, max_features = 5)\n",
    "#RF.fit(X, Y)\n",
    "clf = tree.DecisionTreeClassifier(max_leaf_nodes = 6, max_depth = 8)\n",
    "clf = clf.fit(X, Y)\n",
    "#tree.export_graphviz(clf, out_file='tree.dot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualise the data\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "\n",
    "dot_data = StringIO()\n",
    "\"\"\"export_graphviz(clf, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True)\n",
    "                \"\"\"\n",
    "tree.export_graphviz(clf, out_file=dot_data, class_names=['Sell',\"Hold\",\"Buy\"]\n",
    "                     , filled=True, rounded=True, special_characters = True) \n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue()) \n",
    "graph.write_pdf(\"treey.pdf\") \n",
    "#graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "#Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'round-table.gv.pdf'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rough work\n",
    "#eurusd = eurusd.replace(np.nan, 0)\n",
    "#print(eurusd.tail(25))\n",
    "#eurusd[\"target\"] = eurusd['logret'].iloc[::-1].rolling(2).sum().values[::-1]\n",
    "#print(eurusd.head(10))\n",
    "#data_cols = [col for col in eurusd_train.columns if col not in [\"Date\", \"target\", \"target_binary\"]]\n",
    "#print(data_cols)\n",
    "from graphviz import Digraph\n",
    "\n",
    "dot = Digraph(comment=\"The round table\")\n",
    "dot.node('A', 'King Arthur')\n",
    "dot.node('B', 'Sir Bedevere the Wise')\n",
    "dot.node('L', 'Sir Lancelot the Brave')\n",
    "dot.render('round-table.gv', view=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from model_functions import *\n",
    "import datetime\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "data_file = pd.read_csv(r\"C:\\Users\\edgil\\Documents\\Masters\\dissertation\\code64\\data_set\\CcyData.csv\") # pd.read_csv(r\"/storage/eurusd_train_normed.csv\")\n",
    "data_file = data_file.replace(np.nan, 0)\n",
    "#eurusd_test = eurusd_test.replace(np.nan, 0)\n",
    "performance_store = {\"data_size\" : [], \"Accuracy_Score\" : [], \"ntree\": [],\n",
    "                     \"Info_Ratio\" : [], \"run_time\" : [], \"train_date_st\": [], \"test_date_st\": []}\n",
    "########################### Set Model Paramaters #############################\n",
    "# this looks back over a set period as the memory for the LSTM\n",
    "ntree = 150 # [21, 66]\n",
    "trade_horizon = 24\n",
    "model_features = [\"spot_v_HF\", \"spot_v_MF\", \"spot_v_LF\", \"HF_ema_diff\",\n",
    "                   \"MF_ema_diff\",\"LF_ema_diff\", \"LDN\", \"NY\", \"Asia\", \"target\"]\n",
    "test_buffer = 5\n",
    "data_size = 2500\n",
    "test_split = 0.75\n",
    "total_data_needed = int(data_size*(1 + test_split)) + test_buffer\n",
    "###### Set Targets ##############\n",
    "trade_horizon = 24 # in hours\n",
    "use_risk_adjusted = False # if True: training on the sharpe return else raw\n",
    "data_file[\"target\"] = calculate_target(data_file,trade_horizon,use_risk_adjusted)\n",
    "################### Standardise Entire Dataset using rolling lookback windows ###############\n",
    "features_to_standardise = [\"spot_v_HF\", \"spot_v_MF\", \"spot_v_LF\", \"HF_ema_diff\",\n",
    "                                \"MF_ema_diff\",\"LF_ema_diff\"]\n",
    "# roughly 3 yrs of data slightly less actually\n",
    "window = 15000\n",
    "data_normed = standardise_data(data_file,model_features ,features_to_standardise, window)\n",
    "# add extra features non standardised\n",
    "data_normed['Date'] = data_file['Date'].iloc[window:]\n",
    "data_normed['CCY'] = data_file['CCY'].iloc[window:]\n",
    "#data_normed = data_normed.replace(np.nan, 0)\n",
    "start_row = 0\n",
    "use_separated_chunk = False # Use a rollign window to train and test\n",
    "################ Loop through the full dataset in terms of the training and testing.\n",
    "while start_row < data_file.shape[0]:\n",
    "    # first check if there is enough data left\n",
    "    if (start_row + total_data_needed) > data_file.shape[0]:\n",
    "        # if we are about to go over the limit, then just return the last data_size + test size proportion of data\n",
    "        trunc_data = data_file.iloc[-total_data_needed:,:]\n",
    "    # we need to increment over the data size\n",
    "    if use_separated_chunk:\n",
    "        # this means we jump across the full previous train and test data\n",
    "        trunc_data = data_file.loc[start_row:,:]\n",
    "        start_row += total_data_needed\n",
    "    else:\n",
    "        # this rools the data so that the new training will overlap on the old test set and create a new separated test set\n",
    "        trunc_data = data_file.loc[start_row:,:]\n",
    "        start_row += data_size\n",
    "    # standardise the data\n",
    "    #################### Set up training and testing ########################\n",
    "    print(\"start_row %s\" % start_row)\n",
    "    trunc_data.to_csv(r\"C:\\Users\\edgil\\Documents\\Masters\\dissertation\\code64\\data_set\\TruncData_%s.csv\" % str(start_row)) \n",
    "    # create data_set\n",
    "    train , test = create_train_test_file(trunc_data, data_size, test_split, test_buffer)\n",
    "    train_sample = train[model_features]\n",
    "    test_sample = test[model_features]\n",
    "    # train the model\n",
    "    # verbose = 1 gives the output of the training.\n",
    "    print(\"Starting ML model now end date : %s\" % train['Date'].iloc[-1] )\n",
    "    start_time = datetime.datetime.now()\n",
    "    X = train_sample.iloc[:,:-1]\n",
    "    Y = train_sample[\"target\"]\n",
    "    X_test = test_sample.iloc[:,:-1]\n",
    "    Y_test = test_sample[\"target\"]\n",
    "    # clean the data and nan values\n",
    "    X = X.replace(np.nan, 0)\n",
    "    Y = Y.replace(np.nan, 0)\n",
    "    X_test = X_test.replace(np.nan, 0)\n",
    "    Y_test = Y_test.replace(np.nan, 0)\n",
    "    RF = RandomForestClassifier(ntree = ntree, max_features = 5)\n",
    "    RF.fit(X, Y)\n",
    "    clf = tree.DecisionTreeClassifier(max_leaf_nodes = 6, max_depth = 8)\n",
    "    clf = clf.fit(X, Y)\n",
    "    run_time = datetime.datetime.now() - start_time\n",
    "    # run training on the test data\n",
    "    results = RF.predict_log_proba(X_test)\n",
    "    # The % threshold needed to trigger a signal either way\n",
    "    thold = 0.55\n",
    "    predicted = [signal(i, thold) for i in results]\n",
    "    acc_score = get_accuracy(predicted, test_target)\n",
    "    # This needs to change to handle the change in the target\n",
    "    predictions = pd.DataFrame({\"Date\" : target_dates,\"Predictions\": predicted})\n",
    "    test_results = pd.merge(test,predictions,how=\"left\", on=\"Date\").fillna(0)\n",
    "    # calculate the returns of the signal\n",
    "    test_results[\"scaled_signal\"] = test_results['Predictions'].shift(2).rolling(trade_horizon).sum()/trade_horizon\n",
    "    # no shift needed as we have already done that in previous step\n",
    "    test_results['strat_returns'] = test_results['logret']*test_results['scaled_signal']\n",
    "    test_results['strat_returns_sum'] = test_results['strat_returns'].cumsum()\n",
    "    strat_return = test_results['strat_returns'].sum()\n",
    "    information_ratio = (test_results['strat_returns'].mean()*260)/(test_results['strat_returns'].std()*np.sqrt(260))\n",
    "    \n",
    "    # Store the data as needed\n",
    "    performance_store['data_size'].append(data_size)\n",
    "    performance_store['ntree'].append(ntree)\n",
    "    performance_store['Accuracy_Score'].append(acc_score)\n",
    "    performance_store['Info_Ratio'].append(information_ratio)\n",
    "    performance_store['run_time'].append(run_time)\n",
    "    performance_store['train_date_st'].append(train['Date'].iloc[0])\n",
    "    performance_store['test_date_st'].append(test_results['Date'].iloc[0])\n",
    "    performance_df = pd.DataFrame(performance_store)\n",
    "    save_results = r\"C:\\Users\\edgil\\Documents\\Masters\\dissertation\\code\\dataset\\test_result_start%s_lkbk%s_ntree%s_thold%s.csv\" % (start_row, look_back, ntree, thold)\n",
    "    test_results.to_csv( save_results,index = False)\n",
    "\n",
    "    #### Set up model parameters\n",
    "print(data_normed.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
