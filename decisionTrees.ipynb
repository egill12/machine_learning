{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Decision trees\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "#import graphviz\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the data that we want to create a DT on.\n",
    "# import the fx data , econ and value data for EURUSD.\n",
    "# then create the features (on trend and econ data) standardise and run a DT on the x_train sample.\n",
    "# what is  target? 1 day ahead or long days ahead? trade on binary data.\n",
    "csv_file = {\"FXData\" : r\"C:\\Users\\edgil\\Documents\\Masters\\dissertation\\data\\CurrencyData.csv\",\n",
    "            \"ValueData\" : r\"\",\n",
    "            \"EconData\" : r\"\",\n",
    "            }\n",
    "fxdata = pd.read_csv(csv_file[\"FXData\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this to make sure the dates are imported correctly as UK dates\n",
    "fxdata['Date'] = pd.to_datetime(fxdata['Date'], format= '%d/%m/%Y %H:%M')\n",
    "# Separate out the EURUSD factor\n",
    "eurusd = fxdata[[\"Date\", \"EURUSD\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edgil\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  \nC:\\Users\\edgil\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  # This is added back by InteractiveShellApp.init_path()\nC:\\Users\\edgil\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  if sys.path[0] == '':\nC:\\Users\\edgil\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:13: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  del sys.path[0]\nC:\\Users\\edgil\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:14: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  \nC:\\Users\\edgil\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\ipykernel_launcher.py:15: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# Build out the featur set on price, this may need to be created using funcional process.\n",
    "eurusd[\"logret\"] = np.log(eurusd[\"EURUSD\"]) - np.log(eurusd[\"EURUSD\"].shift(1))\n",
    "# Standardising the daily rets and accumulating the standardised returns, or should we sum the % ret and standardise by its own history\n",
    "# is difference between different accumulated retusn horizons the same as the macd?\n",
    "# should we standardise by the 1 year forward vol?\n",
    "targetlkbk = 5\n",
    "lkbk1M = 22\n",
    "lkbk3M = 66\n",
    "lkbk6M = 132\n",
    "lkbk9M = 198\n",
    "eurusd['1MRet'] = eurusd[\"logret\"].rolling(lkbk1M).sum()\n",
    "eurusd['3MRet'] = eurusd[\"logret\"].rolling(lkbk3M).sum()\n",
    "eurusd['6MRet'] = eurusd[\"logret\"].rolling(lkbk6M).sum()\n",
    "eurusd['9MRet'] = eurusd[\"logret\"].rolling(lkbk9M).sum()\n",
    "eurusd['1Mv3MRet'] = eurusd[\"1MRet\"].sub(eurusd[\"3MRet\"])\n",
    "eurusd['1Mv6MRet'] = eurusd[\"1MRet\"].sub(eurusd[\"6MRet\"])\n",
    "# create a target vector to train on.\n",
    "# must think deeply about what this will look like\n",
    "# to start, this is a 1M forard return calculation, is it right to use overlapping 1M fwd rets? it seems not...\n",
    "eurusd[\"target\"] = eurusd['logret'].iloc[::-1].rolling(targetlkbk).sum().values[::-1]\n",
    "eurusd['target_binary'] = eurusd['target'].apply(np.sign) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very important step is to truncate the data so that we do not see the last 1 year of data.\n",
    "# Q. should we not have a rollign window type of model? or just always aggregate the data from the start?\n",
    "# how ong is testing? we should we the train and test to sizes which make sense to the type of model we use going forward.\n",
    "eurusd = eurusd.loc[eurusd['Date'] < \"2018-01-01 00:00\"]\n",
    "# train size should be at least 5 years?\n",
    "eurusd_train = eurusd.loc[eurusd['Date'] < \"2003-01-01 00:00\"]\n",
    "eurusd_test = eurusd.loc[eurusd['Date'] > \"2010-02-01 00:00\"]\n",
    "# create a target vector to train on.\n",
    "# must think deeply about what this will look like\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18768, 8)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standardise the data using sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# standardise the data now.\n",
    "# remove dates\n",
    "data_cols = [col for col in eurusd_train.columns if col not in [\"Date\", \"target\", \"target_binary\"]]\n",
    "data_train = eurusd_train.loc[:, data_cols]\n",
    "data_test = eurusd_test.loc[:, data_cols]\n",
    "eurusd_train_normed = pd.DataFrame(scaler.fit_transform(data_train), columns = list(data_train.columns))\n",
    "eurusd_test_normed = pd.DataFrame(scaler.transform(data_test), columns = list(data_test.columns)) \n",
    "#print(eurusd_train_normed.tail(50))\n",
    "eurusd_train_normed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### This is where we choose and set up the models!\n",
    "# need to re do the models here.\n",
    "X = eurusd_train_normed[['1Mv3MRet', \"1Mv6MRet\"]]\n",
    "Y = eurusd_train[\"target_binary\"]\n",
    "X_test = eurusd_test_normed\n",
    "Y_test = eurusd_test[\"target_binary\"]\n",
    "# clean the data and nan values\n",
    "X = X.replace(np.nan, 0)\n",
    "Y = Y.replace(np.nan, 0)\n",
    "X_test = X_test.replace(np.nan, 0)\n",
    "Y_test = Y_test.replace(np.nan, 0)\n",
    "#RF = RandomForestClassifier(n_estimators = 150, max_features = 5)\n",
    "#RF.fit(X, Y)\n",
    "clf = tree.DecisionTreeClassifier(max_leaf_nodes = 6, max_depth = 8)\n",
    "clf = clf.fit(X, Y)\n",
    "#tree.export_graphviz(clf, out_file='tree.dot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualise the data\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "\n",
    "dot_data = StringIO()\n",
    "\"\"\"export_graphviz(clf, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True)\n",
    "                \"\"\"\n",
    "tree.export_graphviz(clf, out_file=dot_data, class_names=['Sell',\"Hold\",\"Buy\"]\n",
    "                     , filled=True, rounded=True, special_characters = True) \n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue()) \n",
    "graph.write_pdf(\"treey.pdf\") \n",
    "#graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "#Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'round-table.gv.pdf'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rough work\n",
    "#eurusd = eurusd.replace(np.nan, 0)\n",
    "#print(eurusd.tail(25))\n",
    "#eurusd[\"target\"] = eurusd['logret'].iloc[::-1].rolling(2).sum().values[::-1]\n",
    "#print(eurusd.head(10))\n",
    "#data_cols = [col for col in eurusd_train.columns if col not in [\"Date\", \"target\", \"target_binary\"]]\n",
    "#print(data_cols)\n",
    "from graphviz import Digraph\n",
    "\n",
    "dot = Digraph(comment=\"The round table\")\n",
    "dot.node('A', 'King Arthur')\n",
    "dot.node('B', 'Sir Bedevere the Wise')\n",
    "dot.node('L', 'Sir Lancelot the Brave')\n",
    "dot.render('round-table.gv', view=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edgil\\Documents\\Masters\\dissertation\\code\\model_functions.py:65: RuntimeWarning: divide by zero encountered in true_divide\n  return data_df['logret'].iloc[::-1].shift(2).rolling(trade_horizon).sum().values[::-1]/data_df['logret'].iloc[::-1].shift(2).rolling(trade_horizon).std().values[::-1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_row 25000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ML model now end date : 2006-10-16 19:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.4s finished\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_row 50000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ML model now end date : 2007-12-27 07:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.4s finished\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_row 75000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ML model now end date : 2011-12-23 23:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.5s finished\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_row 100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ML model now end date : 2015-12-22 15:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.5s finished\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from model_functions import *\n",
    "from run_decision_tree import *\n",
    "import datetime\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "file_location = r\"C:\\Users\\edgil\\Documents\\Masters\\dissertation\\code64\\data_set\\CcyData.csv\"\n",
    "performance_store = {\"data_size\" : [], \"Accuracy_Score\" : [], \"ntree\": [],\n",
    "                     \"Info_Ratio\" : [], \"run_time\" : [], \"train_date_st\": [], \"test_date_st\": []}\n",
    "########################### Set Model Paramaters #############################\n",
    "# this looks back over a set period as the memory for the LSTM\n",
    "ntree = 3 # [21, 66]\n",
    "max_features = 5\n",
    "test_buffer = 5\n",
    "data_size = 25000\n",
    "test_split = 0.75\n",
    "# signal threshold, when using classifier\n",
    "thold = 0.55\n",
    "total_data_needed = int(data_size*(1 + test_split)) + test_buffer\n",
    "# standardisation window\n",
    "window = 17500\n",
    "###### Set Targets ##############\n",
    "trade_horizon = 24 # in hours\n",
    "use_risk_adjusted = True # if True: training on the sharpe return else raw\n",
    "use_binary = False # set to true if you are using the risk adjusted and want it binary for classification score\n",
    "use_classifier = False\n",
    "################### Standardise Entire Dataset using rolling lookback windows ###############\n",
    "data_normed, model_features = initialise_process(file_location, trade_horizon, window, use_risk_adjusted)\n",
    "#data_normed = data_normed.replace(np.nan, 0)\n",
    "start_row = 0\n",
    "use_separated_chunk = False # Use a rolling window to train and test\n",
    "################ Loop through the full dataset in terms of the training and testing.\n",
    "while start_row < data_normed.shape[0]:\n",
    "    # first check if there is enough data left\n",
    "    if (start_row + total_data_needed) > data_normed.shape[0]:\n",
    "        # if we are about to go over the limit, then just return the last data_size + test size proportion of data\n",
    "        trunc_data = data_normed.iloc[-total_data_needed:,:]\n",
    "    # we need to increment over the data size\n",
    "    if use_separated_chunk:\n",
    "        # this means we jump across the full previous train and test data\n",
    "        trunc_data = data_normed.loc[start_row:,:]\n",
    "        start_row += total_data_needed\n",
    "    else:\n",
    "        # this rools the data so that the new training will overlap on the old test set and create a new separated test set\n",
    "        trunc_data = data_normed.loc[start_row:,:]\n",
    "        start_row += data_size\n",
    "    # standardise the data\n",
    "    #################### Set up training and testing ########################\n",
    "    print(\"start_row %s\" % start_row)\n",
    "    trunc_data.to_csv(r\"C:\\Users\\edgil\\Documents\\Masters\\dissertation\\code\\dataset\\TruncData_%s.csv\" % str(start_row)) \n",
    "    # create data_set\n",
    "    train , test = create_train_test_file(trunc_data, data_size, test_split, test_buffer)\n",
    "    train_sample = train[model_features]\n",
    "    test_sample = test[model_features]\n",
    "    # train the model\n",
    "    # verbose = 1 gives the output of the training.\n",
    "    print(\"Starting ML model now end date : %s\" % train['Date'].iloc[-1] )\n",
    "    start_time = datetime.datetime.now()\n",
    "    results, acc_score = decision_tree(train_sample, test_sample,use_classifier, use_risk_adjusted,ntree, max_features)\n",
    "    run_time = datetime.datetime.now() - start_time\n",
    "    test_results = backtester(results, test, trade_horizon)[0]\n",
    "    strat_return = backtester(results, test, trade_horizon)[1]\n",
    "    information_ratio = backtester(results, test, trade_horizon)[2]\n",
    "    train_date = train['Date'].iloc[0]\n",
    "    test_date = test['Date'].iloc[0]\n",
    "    performance_df = update_performance(data_size, ntree, acc_score , information_ratio, \n",
    "                                        run_time, train_date, test_date, performance_store)\n",
    "    save_results = r\"C:\\Users\\edgil\\Documents\\Masters\\dissertation\\code\\dataset\\test_result_start%s_useriskadj%s_ntree%s_thold%s.csv\" % (start_row, \n",
    "                    use_risk_adjusted, ntree, thold)\n",
    "    save_perf_df = r\"C:\\Users\\edgil\\Documents\\Masters\\dissertation\\code\\dataset\\performance_df_ntree%s_st_row%s.csv\" % (ntree,start_row) \n",
    "    test_results.to_csv(save_results,index = False)\n",
    "    performance_df.to_csv(save_perf_df,index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edgil\\Documents\\Masters\\dissertation\\code\\model_functions.py:77: RuntimeWarning: divide by zero encountered in true_divide\n  return data_df['logret'].iloc[::-1].shift(2).rolling(trade_horizon).sum().values[::-1]/data_df['logret'].iloc[::-1].shift(2).rolling(trade_horizon).std().values[::-1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_row 1500\nStarting ML model now end date : 2003-01-14 16:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 1500\nstart_row 3000\nStarting ML model now end date : 2003-04-11 04:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 3000\nstart_row 4500\nStarting ML model now end date : 2003-07-08 16:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 4500\nstart_row 6000\nStarting ML model now end date : 2003-10-03 04:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 6000\nstart_row 7500\nStarting ML model now end date : 2003-12-30 16:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 7500\nstart_row 9000\nStarting ML model now end date : 2004-03-26 04:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 9000\nstart_row 10500\nStarting ML model now end date : 2004-06-22 16:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 10500\nstart_row 12000\nStarting ML model now end date : 2004-09-17 04:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 12000\nstart_row 13500\nStarting ML model now end date : 2004-12-14 16:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 13500\nstart_row 15000\nStarting ML model now end date : 2005-03-11 04:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 15000\nstart_row 16500\nStarting ML model now end date : 2005-06-07 16:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 16500\nstart_row 18000\nStarting ML model now end date : 2005-09-02 04:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 18000\nstart_row 19500\nStarting ML model now end date : 2005-11-29 16:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 19500\nstart_row 21000\nStarting ML model now end date : 2006-02-24 04:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 21000\nstart_row 22500\nStarting ML model now end date : 2006-05-23 16:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 22500\nstart_row 24000\nStarting ML model now end date : 2006-08-18 04:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 24000\nstart_row 25500\nStarting ML model now end date : 2006-11-14 16:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 25500\nstart_row 27000\nStarting ML model now end date : 2007-02-09 04:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 27000\nstart_row 28500\nStarting ML model now end date : 2007-05-08 16:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 28500\nstart_row 30000\nStarting ML model now end date : 2007-08-03 04:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 30000\nstart_row 31500\nStarting ML model now end date : 2007-10-30 16:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 31500\nstart_row 33000\nStarting ML model now end date : 2008-01-25 04:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 33000\nstart_row 34500\nStarting ML model now end date : 2008-04-22 16:00:00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3c7fd4f3ec5b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mntree\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mntrees\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         results, acc_score = decision_tree(train_sample, test_sample,use_classifier,\n\u001b[1;32m---> 74\u001b[1;33m                                            use_risk_adjusted,ntree, max_features, max_depth)\n\u001b[0m\u001b[0;32m     75\u001b[0m         \u001b[0mrun_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0mtest_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbacktester\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrade_horizon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Masters\\dissertation\\code\\run_decision_tree.py\u001b[0m in \u001b[0;36mdecision_tree\u001b[1;34m(train, test, use_classifier, use_risk_adjusted, ntree, max_features, max_depth)\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;31m# ass in code for regresion classifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mRF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mntree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_depth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0mRF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m     \u001b[1;31m# run training on the test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    331\u001b[0m                     \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[1;32m--> 333\u001b[1;33m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[0;32m    334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m             \u001b[1;31m# Collect newly grown trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    918\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 920\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    921\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    922\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    757\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[0;32m    117\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'balanced'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    799\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    800\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 801\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m    802\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    803\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    364\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 366\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "##### RUN LOOP on NTREE ####\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from model_functions import *\n",
    "from run_decision_tree import *\n",
    "import datetime\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "file_location = r\"C:\\Users\\edgil\\Documents\\Masters\\dissertation\\code64\\data_set\\CcyData.csv\"\n",
    "performance_store = {\"data_size\" : [], \"Accuracy_Score\" : [], \"ntree\": [],\n",
    "                     \"Info_Ratio\" : [], \"run_time\" : [], \"train_date_st\": [], \"test_date_st\": []}\n",
    "########################### Set Model Paramaters #############################\n",
    "# this looks back over a set period as the memory for the LSTM\n",
    "ntrees = [150] # [i for i in range(25,301,25)] # [21, 66]\n",
    "max_features = 5\n",
    "test_buffer = 5\n",
    "max_depth = 30\n",
    "data_size = 1500 # train every 3 months\n",
    "# I.e. append the data into one df over each training window, but also use all available up until that point\n",
    "concat_results = True \n",
    "# if the number is > 1, then the code takes that as the number of test points you want to use\n",
    "test_split = 460 # roughly one month test ahead\n",
    "# signal threshold, when using classifier\n",
    "thold = 0.55\n",
    "total_data_needed = get_total_data_needed(test_split,data_size,test_buffer)\n",
    "# standardisation window\n",
    "window = 17500\n",
    "###### Set Targets ##############\n",
    "trade_horizon = 24 # in hours\n",
    "use_risk_adjusted = True # if True: training on the sharpe return else raw\n",
    "use_binary = False # set to true if you are using the risk adjusted and want it binary for classification score\n",
    "use_classifier = True\n",
    "################### Standardise Entire Dataset using rolling lookback windows ###############\n",
    "data_normed, model_features = initialise_process(file_location, trade_horizon, window, use_risk_adjusted)\n",
    "#data_normed = data_normed.replace(np.nan, 0)\n",
    "start_row = 0\n",
    "use_separated_chunk = False # Use a rolling window to train and test\n",
    "################ Loop through the full dataset in terms of the training and testing.\n",
    "while start_row < data_normed.shape[0]:\n",
    "    # first check if there is enough data left\n",
    "    if (start_row + total_data_needed) > data_normed.shape[0]:\n",
    "        # if we are about to go over the limit, then just return the last data_size + test size proportion of data\n",
    "        trunc_data = data_normed.iloc[-total_data_needed:,:]\n",
    "    # we need to increment over the data size\n",
    "    if use_separated_chunk:\n",
    "        # this means we jump across the full previous train and test data\n",
    "        trunc_data = data_normed.loc[start_row:,:]\n",
    "        start_row += total_data_needed\n",
    "    if concat_results:\n",
    "        # in this instance, we can to add to the start row first before chunking the data\n",
    "        start_row += data_size\n",
    "        # we are training on all data available up until that point, and testing x timeperiods ahead\n",
    "        trunc_data = data_normed.loc[:start_row,:]  \n",
    "    else:\n",
    "        # this rolls the data so that the new training will overlap on the old test set and create a new separated test set\n",
    "        trunc_data = data_normed.loc[start_row:,:]\n",
    "        start_row += data_size\n",
    "    # standardise the data\n",
    "    #################### Set up training and testing ########################\n",
    "    print(\"start_row %s\" % start_row)\n",
    "    #trunc_data.to_csv(r\"C:\\Users\\edgil\\Documents\\Masters\\dissertation\\code\\dataset\\TruncData_%s.csv\" % str(start_row)) \n",
    "    # create data_set\n",
    "    train , test = create_train_test_file(trunc_data, data_size, test_split, test_buffer, concat_results)\n",
    "    train_sample = train[model_features]\n",
    "    test_sample = test[model_features]\n",
    "    # train the model\n",
    "    # verbose = 1 gives the output of the training.\n",
    "    print(\"Starting ML model now end date : %s\" % train['Date'].iloc[-1] )\n",
    "    start_time = datetime.datetime.now()\n",
    "    for ntree in ntrees:\n",
    "        results, acc_score = decision_tree(train_sample, test_sample,use_classifier,\n",
    "                                           use_risk_adjusted,ntree, max_features, max_depth)\n",
    "        run_time = datetime.datetime.now() - start_time\n",
    "        test_results = backtester(results, test, trade_horizon)[0]\n",
    "        strat_return = backtester(results, test, trade_horizon)[1]\n",
    "        information_ratio = backtester(results, test, trade_horizon)[2]\n",
    "        train_date = train['Date'].iloc[0]\n",
    "        test_date = test['Date'].iloc[0]\n",
    "        performance_df = update_performance(data_size, ntree, acc_score , information_ratio, \n",
    "                                            run_time, train_date, test_date, performance_store)\n",
    "    save_test_df = r\"C:\\Users\\edgil\\Documents\\Masters\\dissertation\\code\\dataset\\testresults_df_ntree%s_st_row%s_use_risk%s_use_SepChunk%s_concat%s.csv\" % (ntree,start_row,use_risk_adjusted,use_separated_chunk,concat_results) \n",
    "    if concat_results:\n",
    "        # i.e. on the first iteration, keep reulsts as is, then we appedn at subsequent iterations\n",
    "        if start_row == data_size:\n",
    "            master_df = test_results\n",
    "        else:\n",
    "            master_df = pd.concat([master_df, test_results], axis = 0).reset_index(drop = True)\n",
    "    else:\n",
    "        test_results.to_csv(save_test_df, index = False)\n",
    "    print(\"finished with row: %s\" % str(start_row))\n",
    "    \n",
    "save_perf_df = r\"C:\\Users\\edgil\\Documents\\Masters\\dissertation\\code\\dataset\\performance_df_ntree%s_st_row%s.csv\" % (ntree,start_row) \n",
    "performance_df.to_csv(save_perf_df,index = False)\n",
    "if concat_results:\n",
    "    master_df.to_csv(save_test_df, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edgil\\Documents\\Masters\\dissertation\\code\\model_functions.py:77: RuntimeWarning: divide by zero encountered in true_divide\n  return data_df['logret'].iloc[::-1].shift(2).rolling(trade_horizon).sum().values[::-1]/data_df['logret'].iloc[::-1].shift(2).rolling(trade_horizon).std().values[::-1]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from model_functions import *\n",
    "from run_decision_tree import *\n",
    "import datetime\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "file_location = r\"C:\\Users\\edgil\\Documents\\Masters\\dissertation\\code64\\data_set\\CcyData.csv\"\n",
    "performance_store = {\"data_size\" : [], \"Accuracy_Score\" : [], \"ntree\": [],\n",
    "                     \"Info_Ratio\" : [], \"run_time\" : [], \"train_date_st\": [], \"test_date_st\": []}\n",
    "########################### Set Model Paramaters #############################\n",
    "# this looks back over a set period as the memory for the LSTM\n",
    "ntrees = [150] # [i for i in range(25,301,25)] # [21, 66]\n",
    "max_features = 5\n",
    "test_buffer = 5\n",
    "max_depth = 30\n",
    "data_size = 1500 # train every 3 months\n",
    "# I.e. append the data into one df over each training window, but also use all available up until that point\n",
    "concat_results = True \n",
    "# if the number is > 1, then the code takes that as the number of test points you want to use\n",
    "test_split = 460 # roughly one month test ahead\n",
    "# signal threshold, when using classifier\n",
    "thold = 0.55\n",
    "total_data_needed = get_total_data_needed(test_split,data_size,test_buffer)\n",
    "# standardisation window\n",
    "window = 17500\n",
    "###### Set Targets ##############\n",
    "trade_horizon = 24 # in hours\n",
    "use_risk_adjusted = True # if True: training on the sharpe return else raw\n",
    "use_binary = False # set to true if you are using the risk adjusted and want it binary for classification score\n",
    "use_classifier = True\n",
    "################### Standardise Entire Dataset using rolling lookback windows ###############\n",
    "data_normed, model_features = initialise_process(file_location, trade_horizon, window, use_risk_adjusted)\n",
    "#data_normed = data_normed.replace(np.nan, 0)\n",
    "start_row = 0\n",
    "use_separated_chunk = False # Use a rolling window to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_row 10000\nStarting ML model now end date : 2004-04-27 11:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 10000\n"
     ]
    }
   ],
   "source": [
    "trunc_data= data_normed.loc[:10000,:]\n",
    "start_row = 10000\n",
    "# standardise the data\n",
    "#################### Set up training and testing ########################\n",
    "print(\"start_row %s\" % start_row)\n",
    "#trunc_data.to_csv(r\"C:\\Users\\edgil\\Documents\\Masters\\dissertation\\code\\dataset\\TruncData_%s.csv\" % str(start_row)) \n",
    "# create data_set\n",
    "train , test = create_train_test_file(trunc_data, data_size, test_split, test_buffer, concat_results)\n",
    "train_sample = train[model_features]\n",
    "test_sample = test[model_features]\n",
    "# train the model\n",
    "# verbose = 1 gives the output of the training.\n",
    "print(\"Starting ML model now end date : %s\" % train['Date'].iloc[-1] )\n",
    "start_time = datetime.datetime.now()\n",
    "for ntree in ntrees:\n",
    "    results, acc_score = decision_tree(train_sample, test_sample,use_classifier,\n",
    "                                       use_risk_adjusted,ntree, max_features, max_depth)\n",
    "    run_time = datetime.datetime.now() - start_time\n",
    "    test_results = backtester(results, test, trade_horizon)[0]\n",
    "    strat_return = backtester(results, test, trade_horizon)[1]\n",
    "    information_ratio = backtester(results, test, trade_horizon)[2]\n",
    "    train_date = train['Date'].iloc[0]\n",
    "    test_date = test['Date'].iloc[0]\n",
    "    performance_df = update_performance(data_size, ntree, acc_score , information_ratio, \n",
    "                                        run_time, train_date, test_date, performance_store)\n",
    "save_test_df = r\"C:\\Users\\edgil\\Documents\\Masters\\dissertation\\code\\dataset\\rf\\testresults_df_ntree%s_st_row%s_use_risk%s_use_SepChunk%s_concat%s.csv\" % (ntree,start_row,use_risk_adjusted,use_separated_chunk,concat_results) \n",
    "if concat_results:\n",
    "    # i.e. on the first iteration, keep reulsts as is, then we appedn at subsequent iterations\n",
    "    if start_row == data_size:\n",
    "        master_df = test_results\n",
    "    else:\n",
    "        master_df = pd.concat([master_df, test_results], axis = 0).reset_index(drop = True)\n",
    "else:\n",
    "    test_results.to_csv(save_test_df, index = False)\n",
    "print(\"finished with row: %s\" % str(start_row))\n",
    "    \n",
    "save_perf_df = r\"C:\\Users\\edgil\\Documents\\Masters\\dissertation\\code\\dataset\\rf\\performance_df_ntree%s_st_row%s.csv\" % (ntree,start_row) \n",
    "performance_df.to_csv(save_perf_df,index = False)\n",
    "if concat_results:\n",
    "    master_df.to_csv(save_test_df, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x10ea7c50>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e89ed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "master_df['strat_returns'].cumsum().plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
