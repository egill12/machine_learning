{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edgil\\Documents\\Masters\\dissertation\\code\\model_functions.py:77: RuntimeWarning: divide by zero encountered in true_divide\n  return data_df['logret'].iloc[::-1].shift(2).rolling(trade_horizon).sum().values[::-1]/data_df['logret'].iloc[::-1].shift(2).rolling(trade_horizon).std().values[::-1]\nC:\\Users\\edgil\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_row 5000\nStarting ML model now end date : 2003-01-14 15:00:00\nfinished with row: 5000\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This code creates and saves the results of using the SVM model across various model features\n",
    "'''\n",
    "\n",
    "##### RUN LOOP on Cost? ####\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from model_functions import *\n",
    "from run_decision_tree import *\n",
    "import datetime\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "file_location = r\"C:\\Users\\edgil\\Documents\\Masters\\dissertation\\code64\\data_set\\CcyData.csv\"\n",
    "performance_store = {\"data_size\" : [], \"Accuracy_Score\" : [], \"ntree\": [],\n",
    "                     \"Info_Ratio\" : [], \"run_time\" : [], \"train_date_st\": [], \"test_date_st\": []}\n",
    "########################### Set Model Paramaters #############################\n",
    "# this looks back over a set period as the memory for the LSTM\n",
    "kernel = \"rbf\" \n",
    "cost = 5 \n",
    "test_buffer = 5\n",
    "data_size = 1500 # train every 3 months\n",
    "# I.e. append the data into one df over each training window, but also use all available up until that point\n",
    "concat_results = False \n",
    "# if the number is > 1, then the code takes that as the number of test points you want to use\n",
    "test_split = 460 # roughly one month test ahead\n",
    "# signal threshold, when using classifier\n",
    "thold = 0.55\n",
    "total_data_needed = get_total_data_needed(test_split,data_size,test_buffer)\n",
    "# standardisation window\n",
    "window = 17500\n",
    "###### Set Targets ##############\n",
    "trade_horizon = 24 # in hours\n",
    "use_risk_adjusted = True # if True: training on the sharpe return else raw\n",
    "use_binary = False # set to true if you are using the risk adjusted and want it binary for classification score\n",
    "use_classifier = True\n",
    "################### Standardise Entire Dataset using rolling lookback windows ###############\n",
    "data_normed, model_features = initialise_process(file_location, trade_horizon, window, use_risk_adjusted)\n",
    "#data_normed = data_normed.replace(np.nan, 0)\n",
    "start_row = 5000\n",
    "use_separated_chunk = False # Use a rolling window to train and test\n",
    "trunc_data = data_normed.loc[:start_row,:]  \n",
    "################ Loop through the full dataset in terms of the training and testing.\n",
    "# standardise the data\n",
    "#################### Set up training and testing ########################\n",
    "print(\"start_row %s\" % start_row)\n",
    "#trunc_data.to_csv(r\"C:\\Users\\edgil\\Documents\\Masters\\dissertation\\code\\dataset\\TruncData_%s.csv\" % str(start_row)) \n",
    "# create data_set\n",
    "train , test = create_train_test_file(trunc_data, data_size, test_split, test_buffer, concat_results)\n",
    "train_sample = train[model_features]\n",
    "test_sample = test[model_features]\n",
    "# train the model\n",
    "# verbose = 1 gives the output of the training.\n",
    "print(\"Starting ML model now end date : %s\" % train['Date'].iloc[-1] )\n",
    "start_time = datetime.datetime.now()\n",
    "results, acc_score = run_svm_model(train_sample, test_sample,use_classifier,\n",
    "                                       use_risk_adjusted,kernel, cost)\n",
    "run_time = datetime.datetime.now() - start_time\n",
    "test_results = backtester(results, test, trade_horizon)[0]\n",
    "strat_return = backtester(results, test, trade_horizon)[1]\n",
    "information_ratio = backtester(results, test, trade_horizon)[2]\n",
    "train_date = train['Date'].iloc[0]\n",
    "test_date = test['Date'].iloc[0]\n",
    "performance_df = update_performance(data_size, cost, acc_score , information_ratio, \n",
    "                                    run_time, train_date, test_date, performance_store)\n",
    "save_test_df = r\"C:\\Users\\edgil\\Documents\\Masters\\dissertation\\code\\dataset\\svm\\testresults_df_st_row%s_cost%s_use_risk%s_use_SepChunk%s_concat%s.csv\" % (start_row,cost,use_risk_adjusted,use_separated_chunk,concat_results) \n",
    "if concat_results:\n",
    "    # i.e. on the first iteration, keep reulsts as is, then we appedn at subsequent iterations\n",
    "    if start_row == data_size:\n",
    "        master_df = test_results\n",
    "    else:\n",
    "        master_df = pd.merge(master_df, test_results, how= \"left\", on= \"Date\").fillna(0)\n",
    "else:\n",
    "    test_results.to_csv(save_test_df, index = False)\n",
    "print(\"finished with row: %s\" % str(start_row))\n",
    "    \n",
    "save_perf_df = r\"C:\\Users\\edgil\\Documents\\Masters\\dissertation\\code\\dataset\\svm\\performance_df_cost%s_st_row%s.csv\" % (cost,start_row) \n",
    "performance_df.to_csv(save_perf_df,index = False)\n",
    "if concat_results:\n",
    "    master_df.to_csv(save_test_df, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_row 1000\nStarting ML model now end date : 2002-11-15 19:00:00\nfinished with row: 1000\nstart_row 1500\nStarting ML model now end date : 2002-12-16 15:00:00\nfinished with row: 1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edgil\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n  \"avoid this warning.\", FutureWarning)\nC:\\Users\\edgil\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_row 2000\nStarting ML model now end date : 2003-01-14 11:00:00\nfinished with row: 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edgil\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_row 2500\nStarting ML model now end date : 2003-02-12 07:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edgil\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 2500\nstart_row 3000\nStarting ML model now end date : 2003-03-13 03:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edgil\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 3000\nstart_row 3500\nStarting ML model now end date : 2003-04-10 23:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edgil\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 3500\nstart_row 4000\nStarting ML model now end date : 2003-05-09 19:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edgil\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_row 4500\nStarting ML model now end date : 2003-06-09 15:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edgil\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 4500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_row 5000\nStarting ML model now end date : 2003-07-08 11:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edgil\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 5000\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This code creates and saves the results of using the SVM model across various model features\n",
    "'''\n",
    "\n",
    "##### RUN LOOP on Cost? ####\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from model_functions import *\n",
    "from run_decision_tree import *\n",
    "import datetime\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "file_location = r\"C:\\Users\\edgil\\Documents\\Masters\\dissertation\\code64\\data_set\\CcyData.csv\"\n",
    "performance_store = {\"data_size\" : [], \"Accuracy_Score\" : [], \"ntree\": [],\n",
    "                     \"Info_Ratio\" : [], \"run_time\" : [], \"train_date_st\": [], \"test_date_st\": []}\n",
    "########################### Set Model Paramaters #############################\n",
    "# this looks back over a set period as the memory for the LSTM\n",
    "kernel = \"rbf\" \n",
    "costs = [5] \n",
    "test_buffer = 5\n",
    "data_size = 500 # train every 3 months ~ 15000\n",
    "# I.e. append the data into one df over each training window, but also use all available up until that point\n",
    "concat_results = True \n",
    "# if the number is > 1, then the code takes that as the number of test points you want to use\n",
    "test_split = 500 # roughly one month test ahead\n",
    "# signal threshold, when using classifier\n",
    "thold = 0.55\n",
    "total_data_needed = get_total_data_needed(test_split,data_size,test_buffer)\n",
    "# standardisation window\n",
    "window = 17500\n",
    "###### Set Targets ##############\n",
    "trade_horizon = 24 # in hours\n",
    "use_risk_adjusted = False # if True: training on the sharpe return else raw\n",
    "use_binary = False # set to true if you are using the risk adjusted and want it binary for classification score\n",
    "use_classifier = True\n",
    "################### Standardise Entire Dataset using rolling lookback windows ###############\n",
    "data_normed, model_features = initialise_process(file_location, trade_horizon, window, use_risk_adjusted)\n",
    "#data_normed = data_normed.replace(np.nan, 0)\n",
    "start_row = data_size\n",
    "use_separated_chunk = False # Use a rolling window to train and test\n",
    "while start_row < data_normed.shape[0]:\n",
    "    # first check if there is enough data left\n",
    "    if (start_row + total_data_needed) > data_normed.shape[0]:\n",
    "        # if we are about to go over the limit, then just return the last data_size + test size proportion of data\n",
    "        trunc_data = data_normed.iloc[-total_data_needed:,:]\n",
    "    # we need to increment over the data size\n",
    "    if use_separated_chunk:\n",
    "        # this means we jump across the full previous train and test data\n",
    "        trunc_data = data_normed.loc[start_row:,:]\n",
    "        start_row += total_data_needed\n",
    "    if concat_results:\n",
    "        # in this instance, we can to add to the start row first before chunking the data\n",
    "        start_row += test_split\n",
    "        # we are training on all data available up until that point, and testing x timeperiods ahead\n",
    "        trunc_data = data_normed.loc[:start_row,:]  \n",
    "    else:\n",
    "        # this rolls the data so that the new training will overlap on the old test set and create a new separated test set\n",
    "        trunc_data = data_normed.loc[start_row:,:]\n",
    "        start_row += data_size\n",
    "    trunc_data.to_csv(r\"C:\\Users\\edgil\\Documents\\Masters\\dissertation\\code\\dataset\\svm\\datatrunc_st_row%s.csv\" % start_row)   \n",
    "    print(\"start_row %s\" % start_row)\n",
    "    #trunc_data.to_csv(r\"C:\\Users\\edgil\\Documents\\Masters\\dissertation\\code\\dataset\\TruncData_%s.csv\" % str(start_row)) \n",
    "    # create data_set\n",
    "    train , test = create_train_test_file(trunc_data, data_size, test_split, test_buffer, concat_results)\n",
    "    train_sample = train[model_features]\n",
    "    test_sample = test[model_features]\n",
    "    # train the model\n",
    "    # verbose = 1 gives the output of the training.\n",
    "    print(\"Starting ML model now end date : %s\" % train['Date'].iloc[-1] )\n",
    "    start_time = datetime.datetime.now()\n",
    "    for cost in costs:\n",
    "        results, acc_score = run_svm_model(train_sample, test_sample,use_classifier,\n",
    "                                           use_risk_adjusted,kernel, cost)\n",
    "        run_time = datetime.datetime.now() - start_time\n",
    "        test_results = backtester(results, test, trade_horizon)[0]\n",
    "        strat_return = backtester(results, test, trade_horizon)[1]\n",
    "        information_ratio = backtester(results, test, trade_horizon)[2]\n",
    "        train_date = train['Date'].iloc[0]\n",
    "        test_date = test['Date'].iloc[0]\n",
    "        performance_df = update_performance(data_size, cost, acc_score , information_ratio, \n",
    "                                        run_time, train_date, test_date, performance_store)\n",
    "    save_test_df = r\"C:\\Users\\edgil\\Documents\\Masters\\dissertation\\code\\dataset\\svm\\testresults_df_st_row%s_cost%s_use_risk%s_use_SepChunk%s_concat%s.csv\" % (start_row,cost,use_risk_adjusted,use_separated_chunk,concat_results) \n",
    "    if concat_results:\n",
    "        # i.e. on the first iteration, keep reulsts as is, then we appedn at subsequent iterations\n",
    "        if start_row == (data_size + test_split):\n",
    "            master_df = test_results\n",
    "        else:\n",
    "            master_df = pd.concat([master_df, test_results], axis = 0).reset_index(drop = True)\n",
    "    else:\n",
    "        test_results.to_csv(save_test_df, index = False)\n",
    "    print(\"finished with row: %s\" % str(start_row))\n",
    "    if start_row == 5000:\n",
    "        break\n",
    "    save_perf_df = r\"C:\\Users\\edgil\\Documents\\Masters\\dissertation\\code\\dataset\\svm\\performance_df_cost%s_st_row%s_userisk%s.csv\" % (cost,start_row, use_risk_adjusted) \n",
    "    performance_df.to_csv(save_perf_df,index = False)\n",
    "if concat_results:\n",
    "    master_df.to_csv(save_test_df, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_row 30000\nStarting ML model now end date : 2007-08-03 03:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edgil\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 30000\nstart_row 45000\nStarting ML model now end date : 2009-12-25 03:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edgil\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 45000\nstart_row 60000\nStarting ML model now end date : 2012-05-18 03:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edgil\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 60000\nstart_row 75000\nStarting ML model now end date : 2014-10-10 03:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edgil\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 75000\nstart_row 90000\nStarting ML model now end date : 2017-03-03 03:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edgil\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 90000\nstart_row 105000\nStarting ML model now end date : 2017-11-30 09:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edgil\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished with row: 105000\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This code creates and saves the results of using the SVM model across various model features\n",
    "'''\n",
    "\n",
    "##### RUN PCA on Cost? ####\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from model_functions import *\n",
    "from run_decision_tree import *\n",
    "import datetime\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "file_location = r\"C:\\Users\\edgil\\Documents\\Masters\\dissertation\\code64\\data_set\\CcyData.csv\"\n",
    "performance_store = {\"data_size\" : [], \"Accuracy_Score\" : [], \"ntree\": [],\n",
    "                     \"Info_Ratio\" : [], \"run_time\" : [], \"train_date_st\": [], \"test_date_st\": []}\n",
    "########################### Set Model Paramaters #############################\n",
    "# this looks back over a set period as the memory for the LSTM\n",
    "kernel = svm_dict['kerel'] \n",
    "costs = svm_dict['cost']\n",
    "test_buffer = 5\n",
    "data_size = 15000 # train every 3 months ~ 15000\n",
    "# I.e. append the data into one df over each training window, but also use all available up until that point\n",
    "concat_results = False \n",
    "# if the number is > 1, then the code takes that as the number of test points you want to use\n",
    "test_split = 0.25 # roughly one month test ahead\n",
    "# signal threshold, when using classifier\n",
    "thold = 0.55\n",
    "total_data_needed = get_total_data_needed(test_split,data_size,test_buffer)\n",
    "# standardisation window\n",
    "window = 17500\n",
    "###### Set Targets ##############\n",
    "trade_horizon = 120 # in hours\n",
    "use_risk_adjusted = False # if True: training on the sharpe return else raw\n",
    "use_binary = False # set to true if you are using the risk adjusted and want it binary for classification score\n",
    "use_classifier = True\n",
    "use_pca = 0 # if = 0 then implies do not use pca in the model\n",
    "################### Standardise Entire Dataset using rolling lookback windows ###############\n",
    "data_normed, model_features, features_to_standardise = initialise_process(file_location, trade_horizon, \n",
    "                                                 window, use_risk_adjusted, use_pca)\n",
    "#data_normed = data_normed.replace(np.nan, 0)\n",
    "start_row = data_size\n",
    "use_separated_chunk = False # Use a rolling window to train and test\n",
    "while start_row < data_normed.shape[0]:\n",
    "    # first check if there is enough data left\n",
    "    if (start_row + total_data_needed) > data_normed.shape[0]:\n",
    "        # if we are about to go over the limit, then just return the last data_size + test size proportion of data\n",
    "        trunc_data = data_normed.iloc[-total_data_needed:,:]\n",
    "    # we need to increment over the data size\n",
    "    if use_separated_chunk:\n",
    "        # this means we jump across the full previous train and test data\n",
    "        trunc_data = data_normed.loc[start_row:,:]\n",
    "        start_row += total_data_needed\n",
    "    if concat_results:\n",
    "        # in this instance, we can to add to the start row first before chunking the data\n",
    "        start_row += test_split\n",
    "        # we are training on all data available up until that point, and testing x timeperiods ahead\n",
    "        trunc_data = data_normed.loc[:start_row,:]  \n",
    "    else:\n",
    "        # this rolls the data so that the new training will overlap on the old test set and create a new separated test set\n",
    "        trunc_data = data_normed.loc[start_row:,:]\n",
    "        start_row += data_size\n",
    "    #trunc_data.to_csv(r\"C:\\Users\\edgil\\Documents\\Masters\\dissertation\\code\\dataset\\svm\\datatrunc_st_row%s.csv\" % start_row)   \n",
    "    print(\"start_row %s\" % start_row)\n",
    "    #trunc_data.to_csv(r\"C:\\Users\\edgil\\Documents\\Masters\\dissertation\\code\\dataset\\TruncData_%s.csv\" % str(start_row)) \n",
    "    # create data_set\n",
    "    train , test = create_train_test_file(trunc_data, data_size, test_split, test_buffer, concat_results)\n",
    "    if use_pca > 0:\n",
    "        train, test, var_explained = get_pca_features(train, test, features_to_standardise, use_pca)\n",
    "    train_sample = train[model_features]\n",
    "    test_sample = test[model_features]\n",
    "    # train the model\n",
    "    # verbose = 1 gives the output of the training.\n",
    "    print(\"Starting ML model now end date : %s\" % train['Date'].iloc[-1] )\n",
    "    start_time = datetime.datetime.now()\n",
    "    for cost in costs:\n",
    "        results, acc_score = run_svm_model(train_sample, test_sample,use_classifier,\n",
    "                                           use_risk_adjusted,kernel, cost)\n",
    "        run_time = datetime.datetime.now() - start_time\n",
    "        test_results = backtester(results, test, trade_horizon)[0]\n",
    "        strat_return = backtester(results, test, trade_horizon)[1]\n",
    "        information_ratio = backtester(results, test, trade_horizon)[2]\n",
    "        train_date = train['Date'].iloc[0]\n",
    "        test_date = test['Date'].iloc[0]\n",
    "        performance_df = update_performance(data_size, cost, acc_score , information_ratio, \n",
    "                                        run_time, train_date, test_date, performance_store)\n",
    "    save_test_df = r\"C:\\Users\\edgil\\Documents\\Masters\\dissertation\\code\\dataset\\svm\\testresults_df_st_row%s_cost%s_use_risk%s_use_SepChunk%s_concat%s_tradeH%s.csv\"  % (start_row,cost,use_risk_adjusted,use_separated_chunk,concat_results,trade_horizon) \n",
    "    if concat_results:\n",
    "        # i.e. on the first iteration, keep reulsts as is, then we appedn at subsequent iterations\n",
    "        if start_row == (data_size + test_split):\n",
    "            master_df = test_results\n",
    "        else:\n",
    "            master_df = pd.concat([master_df, test_results], axis = 0).reset_index(drop = True)\n",
    "    else:\n",
    "        test_results.to_csv(save_test_df, index = False)\n",
    "    print(\"finished with row: %s\" % str(start_row))\n",
    "\n",
    "save_perf_df = r\"C:\\Users\\edgil\\Documents\\Masters\\dissertation\\code\\dataset\\svm\\performance_df_cost%s_st_row%s_userisk%s.csv\" % (cost,start_row, use_risk_adjusted) \n",
    "performance_df.to_csv(save_perf_df,index = False)\n",
    "if concat_results:\n",
    "    master_df.to_csv(save_test_df, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edgil\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "##### FOR PCA features  && train on random data!####\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from model_functions import *\n",
    "from run_decision_tree import *\n",
    "import datetime\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "\n",
    "file_location = r\"C:\\Users\\edgil\\Documents\\Masters\\dissertation\\code64\\data_set\\ccyDataBLIND_STFEATURES.csv\"\n",
    "performance_store = {\"data_size\" : [], \"Accuracy_Score\" : [], \"ntree\": [],\n",
    "                     \"Info_Ratio\" : [], \"run_time\" : [], \"train_date_st\": [], \"test_date_st\": []}\n",
    "params_dict = set_params_random_forests()\n",
    "svm_dict = set_params_svm()\n",
    "use_random_train_data = params_dict['use_random_train_data']\n",
    "########################### Set Model Paramaters #############################\n",
    "# this looks back over a set period as the memory for the LSTM\n",
    "kernel = svm_dict['kernel'] \n",
    "costs = svm_dict['cost']# [i for i in range(25,301,25)] # [21, 66]\n",
    "# if running pca, max features can only be same or less than the full total of features\n",
    "test_buffer = params_dict['test_buffer']\n",
    "data_size = params_dict['data_size'] #  initially using 1500 training points\n",
    "# I.e. append the data into one df over each training window, but also use all available up until that point\n",
    "concat_results = params_dict['concat_results']\n",
    "# if the number is > 1, then the code takes that as the number of test points you want to use\n",
    "test_split = params_dict['test_split'] # roughly one month test ahead, which is a one month retrain period\n",
    "# signal threshold, when using classifier\n",
    "thold = params_dict['thold']\n",
    "total_data_needed = get_total_data_needed(test_split,data_size,test_buffer)\n",
    "# standardisation window\n",
    "window = params_dict['window']\n",
    "###### Set Targets ##############\n",
    "trade_horizon = params_dict['trade_horizon'] # in hours\n",
    "use_risk_adjusted = params_dict['use_risk_adjusted'] # if True: training on the sharpe return else raw\n",
    "use_binary = params_dict['use_binary'] # set to true if you are using the risk adjusted and want it binary for classification score\n",
    "use_classifier = params_dict['use_classifier']\n",
    "use_pca = params_dict['use_pca'] # if = 0 then implies do not use pca in the model\n",
    "use_separated_chunk = params_dict['use_separated_chunk']\n",
    "################### Standardise Entire Dataset using rolling lookback windows ###############\n",
    "data_normed, model_features, features_to_standardise = initialise_process(file_location, trade_horizon, \n",
    "                                                 window, use_risk_adjusted, use_pca,use_random_train_data)\n",
    "#data_normed = data_normed.replace(np.nan, 0)\n",
    "start_row = data_size\n",
    " # Use a rolling window to train and test\n",
    "################ Loop through the full dataset in terms of the training and testing.\n",
    "if use_random_train_data:\n",
    "    random_data_location = r\"C:\\Users\\edgil\\Documents\\Masters\\dissertation\\code64\\data_set\\CcyRandomTrend.csv\"\n",
    "    train, model_features, features_to_standardise = initialise_process(random_data_location, trade_horizon, \n",
    "                                                 window, use_risk_adjusted, use_pca, use_random_train_data)\n",
    "    test, model_features, features_to_standardise  = initialise_process(file_location, trade_horizon, \n",
    "                                                 window, use_risk_adjusted, use_pca, \n",
    "                                                use_random_train_data= False)\n",
    "    if use_pca > 0:\n",
    "        train, test, var_explained = get_pca_features(train,test, features_to_standardise, use_pca)\n",
    "    train_sample = train[model_features]\n",
    "    test_sample = test[model_features]\n",
    "    for cost in costs:\n",
    "        results, acc_score = run_svm_model(train_sample, test_sample,use_classifier, use_risk_adjusted,kernel,cost)\n",
    "        test_results = backtester(results, test, trade_horizon)[0]\n",
    "        strat_return = backtester(results, test, trade_horizon)[1]\n",
    "        information_ratio = backtester(results, test, trade_horizon)[2]\n",
    "        train_date = 1 # train['Date'].iloc[0]\n",
    "        test_date = test['Date'].iloc[0]\n",
    "        run_time = \"random\"\n",
    "        performance_df = update_performance(data_size, cost, acc_score , information_ratio, \n",
    "                                            run_time, train_date, test_date, performance_store)\n",
    "    save_test_df = r\"C:\\Users\\edgil\\Documents\\Masters\\dissertation\\code\\dataset\\svm\\randomData%s_COST%s_use_risk%s_use_SepChunk%s_concat%s_TH%s.csv\" % (kernel,\n",
    "                                    cost,use_risk_adjusted,use_separated_chunk,concat_results, trade_horizon) \n",
    "    test_results.to_csv(save_test_df, index = False)\n",
    "else:\n",
    "    # if not using random data then move to the normal method.\n",
    "    while start_row < data_normed.shape[0]:\n",
    "        # first check if there is enough data left\n",
    "        if (start_row + total_data_needed) > data_normed.shape[0]:\n",
    "            # if we are about to go over the limit, then just return the last data_size + test size proportion of data\n",
    "            trunc_data = data_normed.iloc[-total_data_needed:,:]\n",
    "        # we need to increment over the data size\n",
    "        if use_separated_chunk:\n",
    "            # this means we jump across the full previous train and test data\n",
    "            trunc_data = data_normed.loc[start_row:,:]\n",
    "            start_row += total_data_needed\n",
    "        if concat_results:\n",
    "            # in this instance, we can to add to the start row first before chunking the data\n",
    "            start_row += test_split\n",
    "            # we are training on all data available up until that point, and testing x timeperiods ahead\n",
    "            trunc_data = data_normed.loc[:start_row,:]  \n",
    "        else:\n",
    "            # this rolls the data so that the new training will overlap on the old test set and create a new separated test set\n",
    "            trunc_data = data_normed.loc[start_row:,:]\n",
    "            start_row += data_size\n",
    "        #trunc_data.to_csv(r\"C:\\Users\\edgil\\Documents\\Masters\\dissertation\\code\\dataset\\svm\\datatrunc_st_row%s.csv\" % start_row)   \n",
    "        print(\"start_row %s\" % start_row)\n",
    "        #trunc_data.to_csv(r\"C:\\Users\\edgil\\Documents\\Masters\\dissertation\\code\\dataset\\TruncData_%s.csv\" % str(start_row)) \n",
    "        # create data_set\n",
    "        train , test = create_train_test_file(trunc_data, data_size, test_split, test_buffer, concat_results)\n",
    "        if use_pca > 0:\n",
    "            train, test, var_explained = get_pca_features(train, test, features_to_standardise, use_pca)\n",
    "        train_sample = train[model_features]\n",
    "        test_sample = test[model_features]\n",
    "        # train the model\n",
    "        # verbose = 1 gives the output of the training.\n",
    "        print(\"Starting ML model now end date : %s\" % train['Date'].iloc[-1] )\n",
    "        start_time = datetime.datetime.now()\n",
    "        for cost in costs:\n",
    "            results, acc_score = run_svm_model(train_sample, test_sample,use_classifier,\n",
    "                                               use_risk_adjusted,kernel, cost)\n",
    "            run_time = datetime.datetime.now() - start_time\n",
    "            test_results = backtester(results, test, trade_horizon)[0]\n",
    "            strat_return = backtester(results, test, trade_horizon)[1]\n",
    "            information_ratio = backtester(results, test, trade_horizon)[2]\n",
    "            train_date = train['Date'].iloc[0]\n",
    "            test_date = test['Date'].iloc[0]\n",
    "            performance_df = update_performance(data_size, cost, acc_score , information_ratio, \n",
    "                                            run_time, train_date, test_date, performance_store)\n",
    "        save_test_df = r\"C:\\Users\\edgil\\Documents\\Masters\\dissertation\\code\\dataset\\svm\\testresults_df_st_row%s_cost%s_use_risk%s_use_SepChunk%s_concat%s_tradeH%s.csv\"  % (start_row,cost,use_risk_adjusted,use_separated_chunk,concat_results,trade_horizon) \n",
    "        if concat_results:\n",
    "            # i.e. on the first iteration, keep reulsts as is, then we appedn at subsequent iterations\n",
    "            if start_row == (data_size + test_split):\n",
    "                master_df = test_results\n",
    "            else:\n",
    "                master_df = pd.concat([master_df, test_results], axis = 0).reset_index(drop = True)\n",
    "        else:\n",
    "            test_results.to_csv(save_test_df, index = False)\n",
    "        print(\"finished with row: %s\" % str(start_row))\n",
    "    \n",
    "    save_perf_df = r\"C:\\Users\\edgil\\Documents\\Masters\\dissertation\\code\\dataset\\svm\\performance_df_cost%s_st_row%s_userisk%s.csv\" % (cost,start_row, use_risk_adjusted) \n",
    "    performance_df.to_csv(save_perf_df,index = False)\n",
    "    if concat_results:\n",
    "        master_df.to_csv(save_test_df, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
