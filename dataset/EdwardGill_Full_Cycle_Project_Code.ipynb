{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This Code is to be run in chronological order and it includes the full lifecycle of the code in the following stages\n",
    "1. Get raw data and process \n",
    "2. Get specified features\n",
    "3. Run the decision tree model and save results\n",
    "4. Run the SVm model and save results.\n",
    "5. Run LSTM (must be done on a 64bit machine which has Keras installed\n",
    "6. Run the Trend estimation \n",
    "7. Save the results.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this file creates the features used in the trend RNN.\n",
    "# features will revolve around high frequency (intra-day) mid-freq (daily to weekly), long term ( monthly trends)\n",
    "# for the intraday factor, should we try for minutely data?\n",
    "import pandas as pd\n",
    "import jupyter\n",
    "import numpy as np\n",
    "import datetime \n",
    "from scipy.stats import norm\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "# Set up the data that we want to create a DT on.\n",
    "# import the fx data , econ and value data for EURUSD.\n",
    "# then create the features (on trend and econ data) standardise and run a DT on the x_train sample.\n",
    "# what is  target? 1 day ahead or long days ahead? trade on binary data.\n",
    "csv_file = {\"FXData\" : r\"CurrencyData.csv\",\n",
    "            \"ValueData\" : r\"\",\n",
    "            \"EconData\" : r\"\",\n",
    "            }\n",
    "fxdata = pd.read_csv(csv_file[\"FXData\"])\n",
    "fxdata['Date'] = pd.to_datetime(fxdata['Date'], format= '%d/%m/%Y %H:%M')\n",
    "# Separate out the EURUSD factor `\n",
    "eurusd = fxdata[[\"Date\", \"EURUSD\"]]\n",
    "def get_time(row):\n",
    "    '''\n",
    "    Get the time stamp of the day \n",
    "    '''\n",
    "    return row.time()\n",
    "eurusd['timestamp'] = eurusd['Date'].apply(get_time)\n",
    "df = eurusd\n",
    "# Very important step is to truncate the data so that we do not see the last 1 year of data.\n",
    "# Q. should we not have a rolling window type of model? or just always aggregate the data from the start?\n",
    "# how long is testing? we should we the train and test to sizes which make sense to the type of model we use going forward.\n",
    "# Making it full sample by commenting out the below\n",
    "#eurusd = eurusd.loc[eurusd['Date'] < \"2018-01-01 00:00\"]\n",
    "# create a target vector to train on.\n",
    "# What is the target? binary return? sharpe ratio? This can then help us to position size the trade,\n",
    "# Build out the feature set on price, this may need to be created using functional process.\n",
    "eurusd[\"logret\"] = np.log(eurusd[\"EURUSD\"]) - np.log(eurusd[\"EURUSD\"].shift(1))\n",
    "# Standardising the daily rets and accumulating the standardised returns, or should we sum the % ret and standardise by its own history\n",
    "# is difference between different accumulated retusn horizons the same as the macd?\n",
    "# should we standardise by the 1 year forward vol?\n",
    "# Use uncommented features if you want to use the short term features\n",
    "short = 21 #\n",
    "medium = 55 #15\n",
    "long = 100 #55\n",
    "longest = 200 # 100\n",
    "\n",
    "# TODO: Should this be an EMA or simple average? Using EWMA now as we \n",
    "# overweight recent history\n",
    "eurusd['HF_short'] = eurusd[\"EURUSD\"].ewm(short).mean()\n",
    "eurusd['HF_medium'] = eurusd[\"EURUSD\"].ewm(medium).mean()\n",
    "eurusd['HF_long'] = eurusd[\"EURUSD\"].ewm(long).mean()\n",
    "eurusd['HF_longest'] = eurusd[\"EURUSD\"].ewm(longest).mean()\n",
    "# differences to spot\n",
    "eurusd['spot_v_HF_short'] = eurusd[\"EURUSD\"] - eurusd['HF_short']\n",
    "eurusd['spot_v_HF_medium'] = eurusd[\"EURUSD\"] - eurusd['HF_medium']\n",
    "eurusd['spot_v_HF_long'] = eurusd[\"EURUSD\"] - eurusd['HF_long']\n",
    "eurusd['spot_v_HF_longest'] = eurusd[\"EURUSD\"] - eurusd['HF_longest'] \n",
    "\n",
    "# medium frequency factors, multiplyer allows us to scale up the lookback as needed.\n",
    "# days to weeks\n",
    "medium_multiplyer = 24\n",
    "eurusd['MF_short'] = eurusd[\"EURUSD\"].ewm(short*medium_multiplyer).mean()\n",
    "eurusd['MF_medium'] = eurusd[\"EURUSD\"].ewm(medium*medium_multiplyer).mean()\n",
    "eurusd['MF_long'] = eurusd[\"EURUSD\"].ewm(long*medium_multiplyer).mean()\n",
    "eurusd['MF_longest'] = eurusd[\"EURUSD\"].ewm(longest*medium_multiplyer).mean()\n",
    "# differences to spot\n",
    "# to measure relative momentum\n",
    "eurusd['spot_v_MF_short'] = eurusd[\"EURUSD\"] - eurusd['MF_short']\n",
    "eurusd['spot_v_MF_medium'] = eurusd[\"EURUSD\"] - eurusd['MF_medium']\n",
    "eurusd['spot_v_MF_long'] = eurusd[\"EURUSD\"] - eurusd['MF_long']\n",
    "eurusd['spot_v_MF_longest'] = eurusd[\"EURUSD\"] - eurusd['MF_longest'] \n",
    "# long term factors\n",
    "# weeks to months\n",
    "long_multiplyer = 120 # each period is now one business week, 24*5\n",
    "eurusd['LF_short'] = eurusd[\"EURUSD\"].ewm(short*long_multiplyer).mean()\n",
    "eurusd['LF_medium'] = eurusd[\"EURUSD\"].ewm(medium*long_multiplyer).mean()\n",
    "eurusd['LF_long'] = eurusd[\"EURUSD\"].ewm(long*long_multiplyer).mean()\n",
    "eurusd['LF_longest'] = eurusd[\"EURUSD\"].ewm(longest*long_multiplyer).mean()\n",
    "# differences to spot\n",
    "# to measure relative momentum\n",
    "eurusd['spot_v_LF_short'] = eurusd[\"EURUSD\"] - eurusd['LF_short']\n",
    "eurusd['spot_v_LF_medium'] = eurusd[\"EURUSD\"] - eurusd['LF_medium']\n",
    "eurusd['spot_v_LF_long'] = eurusd[\"EURUSD\"] - eurusd['LF_long']\n",
    "eurusd['spot_v_LF_longest'] = eurusd[\"EURUSD\"] - eurusd['LF_longest'] \n",
    "\n",
    "# average of both spot distance and each ema distance\n",
    "# take simple average of the divergences at each time frame\n",
    "eurusd['spot_v_HF'] = (eurusd['spot_v_HF_short'] + eurusd['spot_v_HF_medium'] + eurusd['spot_v_HF_long'] + eurusd['spot_v_HF_longest'])/4\n",
    "eurusd['spot_v_MF'] = (eurusd['spot_v_MF_short'] + eurusd['spot_v_MF_medium'] + eurusd['spot_v_MF_long'] + eurusd['spot_v_MF_longest'])/4\n",
    "eurusd['spot_v_LF'] = (eurusd['spot_v_LF_short'] + eurusd['spot_v_LF_medium'] + eurusd['spot_v_LF_long'] + eurusd['spot_v_LF_longest'])/4 \n",
    "#differences to each ema\n",
    "# This can capture the divergences between the EMAs, which allows us to grasp the speed of the move\n",
    "eurusd['HF_ema_diff'] = (eurusd['HF_short']-eurusd['HF_medium']) + (eurusd['HF_medium']-eurusd['HF_long']) + (eurusd['HF_long']-eurusd['HF_longest'])\n",
    "eurusd['MF_ema_diff'] = (eurusd['MF_short']-eurusd['MF_medium']) + (eurusd['MF_medium']-eurusd['MF_long']) + (eurusd['MF_long']-eurusd['MF_longest'])\n",
    "eurusd['LF_ema_diff'] = (eurusd['LF_short']-eurusd['LF_medium']) + (eurusd['LF_medium']-eurusd['LF_long']) + (eurusd['LF_long']-eurusd['LF_longest'])\n",
    "# Add in hourly feature times. Think this is important as there can be certain patterns that occur into and out\n",
    "# of these time frames\n",
    "# London and NY liquid hours\n",
    "eurusd['LDN'] = 0\n",
    "eurusd['NY'] = 0\n",
    "eurusd['Asia'] = 0\n",
    "# adding in timezone changes\n",
    "eurusd['LDN'].loc[(eurusd[\"timestamp\"] >= datetime.time(7,0)) & (eurusd[\"timestamp\"] <= datetime.time(12,0))] = 1\n",
    "eurusd['LDN'].loc[(eurusd[\"timestamp\"] >= datetime.time(13,0)) & (eurusd[\"timestamp\"] <= datetime.time(17,0))] = 0.5\n",
    "eurusd['NY'].loc[(eurusd[\"timestamp\"] >= datetime.time(13,0)) & (eurusd[\"timestamp\"] <= datetime.time(17,0))] = 0.5\n",
    "eurusd['NY'].loc[(eurusd[\"timestamp\"] >= datetime.time(18,0)) & (eurusd[\"timestamp\"] <= datetime.time(22,0))] = 1\n",
    "eurusd['Asia'].loc[(eurusd[\"timestamp\"] >= datetime.time(23,0))] = 1\n",
    "eurusd['Asia'].loc[(eurusd[\"timestamp\"] <= datetime.time(6,0))] = 1\n",
    "\n",
    "# Now adding the target vector\n",
    "targetlkbk = 24\n",
    "# Using a shift = 2 so that the forward return starts from exactly the next future time step.\n",
    "eurusd[\"target\"] = eurusd['logret'].iloc[::-1].shift(2).rolling(targetlkbk).sum().values[::-1]\n",
    "eurusd['target_binary'] = eurusd['target'].apply(np.sign) \n",
    "eurusd['CCY'] = eurusd['EURUSD']\n",
    "eurusd.to_csv(r\"ccyDataFullSampleLongTerm.csv\"\n",
    "                           , index = False)\n",
    "'''\n",
    "----------------Create for Short term Features as well-------\n",
    "'''\n",
    "# Use uncommented features if you want to use the short term features\n",
    "short = 5\n",
    "medium = 15\n",
    "long = 55\n",
    "longest =  100\n",
    "\n",
    "# TODO: Should this be an EMA or simple average? Using EWMA now as we \n",
    "# overweight recent history\n",
    "eurusd['HF_short'] = eurusd[\"EURUSD\"].ewm(short).mean()\n",
    "eurusd['HF_medium'] = eurusd[\"EURUSD\"].ewm(medium).mean()\n",
    "eurusd['HF_long'] = eurusd[\"EURUSD\"].ewm(long).mean()\n",
    "eurusd['HF_longest'] = eurusd[\"EURUSD\"].ewm(longest).mean()\n",
    "# differences to spot\n",
    "eurusd['spot_v_HF_short'] = eurusd[\"EURUSD\"] - eurusd['HF_short']\n",
    "eurusd['spot_v_HF_medium'] = eurusd[\"EURUSD\"] - eurusd['HF_medium']\n",
    "eurusd['spot_v_HF_long'] = eurusd[\"EURUSD\"] - eurusd['HF_long']\n",
    "eurusd['spot_v_HF_longest'] = eurusd[\"EURUSD\"] - eurusd['HF_longest'] \n",
    "\n",
    "# medium frequency factors, multiplyer allows us to scale up the lookback as needed.\n",
    "# days to weeks\n",
    "medium_multiplyer = 24\n",
    "eurusd['MF_short'] = eurusd[\"EURUSD\"].ewm(short*medium_multiplyer).mean()\n",
    "eurusd['MF_medium'] = eurusd[\"EURUSD\"].ewm(medium*medium_multiplyer).mean()\n",
    "eurusd['MF_long'] = eurusd[\"EURUSD\"].ewm(long*medium_multiplyer).mean()\n",
    "eurusd['MF_longest'] = eurusd[\"EURUSD\"].ewm(longest*medium_multiplyer).mean()\n",
    "# differences to spot\n",
    "# to measure relative momentum\n",
    "eurusd['spot_v_MF_short'] = eurusd[\"EURUSD\"] - eurusd['MF_short']\n",
    "eurusd['spot_v_MF_medium'] = eurusd[\"EURUSD\"] - eurusd['MF_medium']\n",
    "eurusd['spot_v_MF_long'] = eurusd[\"EURUSD\"] - eurusd['MF_long']\n",
    "eurusd['spot_v_MF_longest'] = eurusd[\"EURUSD\"] - eurusd['MF_longest'] \n",
    "# long term factors\n",
    "# weeks to months\n",
    "long_multiplyer = 120 # each period is now one business week, 24*5\n",
    "eurusd['LF_short'] = eurusd[\"EURUSD\"].ewm(short*long_multiplyer).mean()\n",
    "eurusd['LF_medium'] = eurusd[\"EURUSD\"].ewm(medium*long_multiplyer).mean()\n",
    "eurusd['LF_long'] = eurusd[\"EURUSD\"].ewm(long*long_multiplyer).mean()\n",
    "eurusd['LF_longest'] = eurusd[\"EURUSD\"].ewm(longest*long_multiplyer).mean()\n",
    "# differences to spot\n",
    "# to measure relative momentum\n",
    "eurusd['spot_v_LF_short'] = eurusd[\"EURUSD\"] - eurusd['LF_short']\n",
    "eurusd['spot_v_LF_medium'] = eurusd[\"EURUSD\"] - eurusd['LF_medium']\n",
    "eurusd['spot_v_LF_long'] = eurusd[\"EURUSD\"] - eurusd['LF_long']\n",
    "eurusd['spot_v_LF_longest'] = eurusd[\"EURUSD\"] - eurusd['LF_longest'] \n",
    "\n",
    "# average of both spot distance and each ema distance\n",
    "# take simple average of the divergences at each time frame\n",
    "eurusd['spot_v_HF'] = (eurusd['spot_v_HF_short'] + eurusd['spot_v_HF_medium'] + eurusd['spot_v_HF_long'] + eurusd['spot_v_HF_longest'])/4\n",
    "eurusd['spot_v_MF'] = (eurusd['spot_v_MF_short'] + eurusd['spot_v_MF_medium'] + eurusd['spot_v_MF_long'] + eurusd['spot_v_MF_longest'])/4\n",
    "eurusd['spot_v_LF'] = (eurusd['spot_v_LF_short'] + eurusd['spot_v_LF_medium'] + eurusd['spot_v_LF_long'] + eurusd['spot_v_LF_longest'])/4 \n",
    "#differences to each ema\n",
    "# This can capture the divergences between the EMAs, which allows us to grasp the speed of the move\n",
    "eurusd['HF_ema_diff'] = (eurusd['HF_short']-eurusd['HF_medium']) + (eurusd['HF_medium']-eurusd['HF_long']) + (eurusd['HF_long']-eurusd['HF_longest'])\n",
    "eurusd['MF_ema_diff'] = (eurusd['MF_short']-eurusd['MF_medium']) + (eurusd['MF_medium']-eurusd['MF_long']) + (eurusd['MF_long']-eurusd['MF_longest'])\n",
    "eurusd['LF_ema_diff'] = (eurusd['LF_short']-eurusd['LF_medium']) + (eurusd['LF_medium']-eurusd['LF_long']) + (eurusd['LF_long']-eurusd['LF_longest'])\n",
    "# Add in hourly feature times. Think this is important as there can be certain patterns that occur into and out\n",
    "# of these time frames\n",
    "# London and NY liquid hours\n",
    "eurusd['LDN'] = 0\n",
    "eurusd['NY'] = 0\n",
    "eurusd['Asia'] = 0\n",
    "# adding in timezone changes\n",
    "eurusd['LDN'].loc[(eurusd[\"timestamp\"] >= datetime.time(7,0)) & (eurusd[\"timestamp\"] <= datetime.time(12,0))] = 1\n",
    "eurusd['LDN'].loc[(eurusd[\"timestamp\"] >= datetime.time(13,0)) & (eurusd[\"timestamp\"] <= datetime.time(17,0))] = 0.5\n",
    "eurusd['NY'].loc[(eurusd[\"timestamp\"] >= datetime.time(13,0)) & (eurusd[\"timestamp\"] <= datetime.time(17,0))] = 0.5\n",
    "eurusd['NY'].loc[(eurusd[\"timestamp\"] >= datetime.time(18,0)) & (eurusd[\"timestamp\"] <= datetime.time(22,0))] = 1\n",
    "eurusd['Asia'].loc[(eurusd[\"timestamp\"] >= datetime.time(23,0))] = 1\n",
    "eurusd['Asia'].loc[(eurusd[\"timestamp\"] <= datetime.time(6,0))] = 1\n",
    "\n",
    "# Now adding the target vector\n",
    "targetlkbk = 24\n",
    "# Using a shift = 2 so that the forward return starts from exactly the next future time step.\n",
    "eurusd[\"target\"] = eurusd['logret'].iloc[::-1].shift(2).rolling(targetlkbk).sum().values[::-1]\n",
    "eurusd['target_binary'] = eurusd['target'].apply(np.sign) \n",
    "eurusd['CCY'] = eurusd['EURUSD']\n",
    "eurusd.to_csv(r\"ccyData.csv\", index = False)\n",
    "'''\n",
    "---------------- This code plots the EURUSD and Moving Averages -------\n",
    "'''\n",
    "# This code plots the chart of EURUSD and its moving averages\n",
    "from matplotlib import pyplot as plt\n",
    "eurusd['DDMMYY'] = eurusd['Date'].dt.date\n",
    "fig = plt.figure(1,figsize=(10,8))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(pd.to_datetime(eurusd['Date']),eurusd['EURUSD'], c = \"b\")\n",
    "ax.plot(pd.to_datetime(eurusd['Date']),eurusd['MF_medium'], c = \"g\")\n",
    "ax.plot(pd.to_datetime(eurusd['Date']),eurusd['LF_medium'], c = \"r\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"1 EUR in USD Terms\")\n",
    "ax.legend(['EURUSD', 'Medium Term Moving Average', \"Long Term Moving Average\"])\n",
    "ax.get_figure().savefig(\"EURUSDMovingAvg.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "2. The decision tree code\n",
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Decision trees\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "#import graphviz\n",
    "import os\n",
    "\n",
    "'''\n",
    "----- This code shows the running of the decision tree model on either running the random data train or the historical price\n",
    "'''\n",
    "##### FOR PCA features  && train on random data!####\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from model_functions import *\n",
    "from run_decision_tree import *\n",
    "import datetime\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "\n",
    "# if you want to test on thw withheld data then use file name BLIND_STFEATURES.csv\n",
    "\n",
    "file_location = r\"ccyData.csv\"\n",
    "performance_store = {\"data_size\" : [], \"Accuracy_Score\" : [], \"ntree\": [],\n",
    "                     \"Info_Ratio\" : [], \"run_time\" : [], \"train_date_st\": [], \"test_date_st\": []}\n",
    "params_dict = set_params_random_forests()\n",
    "use_random_train_data = params_dict['use_random_train_data']\n",
    "########################### Set Model Paramaters #############################\n",
    "# this looks back over a set period as the memory for the LSTM\n",
    "'''\n",
    "----- If you want to loop of the number of trees to use, then uncomment the next line..--------\n",
    "'''\n",
    "\n",
    "ntrees = params_dict['ntrees'] # [i for i in range(25,301,25)] # [21, 66]\n",
    "# if running pca, max features can only be same or less than the full total of features\n",
    "max_features = params_dict['max_features']\n",
    "test_buffer = params_dict['test_buffer']\n",
    "max_depth = params_dict['max_depth']\n",
    "data_size = params_dict['data_size'] #  initially using 1500 training points\n",
    "# I.e. append the data into one df over each training window, but also use all available up until that point\n",
    "concat_results = params_dict['concat_results']\n",
    "# if the number is > 1, then the code takes that as the number of test points you want to use\n",
    "test_split = params_dict['test_split'] # roughly one month test ahead, which is a one month retrain period\n",
    "# signal threshold, when using classifier\n",
    "thold = params_dict['thold']\n",
    "total_data_needed = get_total_data_needed(test_split,data_size,test_buffer)\n",
    "# standardisation window\n",
    "window = params_dict['window']\n",
    "###### Set Targets ##############\n",
    "trade_horizon = params_dict['trade_horizon'] # in hours\n",
    "use_risk_adjusted = params_dict['use_risk_adjusted'] # if True: training on the sharpe return else raw\n",
    "use_binary = params_dict['use_binary'] # set to true if you are using the risk adjusted and want it binary for classification score\n",
    "use_classifier = params_dict['use_classifier']\n",
    "use_pca = params_dict['use_pca'] # if = 0 then implies do not use pca in the model\n",
    "use_separated_chunk = params_dict['use_separated_chunk']\n",
    "################### Standardise Entire Dataset using rolling lookback windows ###############\n",
    "data_normed, model_features, features_to_standardise = initialise_process(file_location, trade_horizon, \n",
    "                                                 window, use_risk_adjusted, use_pca,use_random_train_data)\n",
    "#data_normed = data_normed.replace(np.nan, 0)\n",
    "start_row = data_size\n",
    " # Use a rolling window to train and test\n",
    "################ Loop through the full dataset in terms of the training and testing.\n",
    "if use_random_train_data:\n",
    "    random_data_location = r\"CcyRandomTrend.csv\"\n",
    "    train, model_features, features_to_standardise = initialise_process(random_data_location, trade_horizon, \n",
    "                                                 window, use_risk_adjusted, use_pca, use_random_train_data)\n",
    "    test, model_features, features_to_standardise  = initialise_process(file_location, trade_horizon, \n",
    "                                                 window, use_risk_adjusted, use_pca, \n",
    "                                                use_random_train_data = False)\n",
    "    if use_pca > 0:\n",
    "        train, test, var_explained = get_pca_features(train, test, features_to_standardise, use_pca)\n",
    "    train_sample = train[model_features]\n",
    "    test_sample = test[model_features]\n",
    "    for ntree in ntrees:\n",
    "        results, acc_score = decision_tree(train_sample, test_sample,use_classifier,\n",
    "                                               use_risk_adjusted,ntree, max_features, max_depth)\n",
    "        test_results = backtester(results, test, trade_horizon)[0]\n",
    "        strat_return = backtester(results, test, trade_horizon)[1]\n",
    "        information_ratio = backtester(results, test, trade_horizon)[2]\n",
    "        train_date = 1 # train['Date'].iloc[0]\n",
    "        test_date = test['Date'].iloc[0]\n",
    "        run_time = \"random\"\n",
    "        performance_df = update_performance(data_size, ntree, acc_score , information_ratio, \n",
    "                                            run_time, train_date, test_date, performance_store)\n",
    "    save_test_df = r\"randomData%s_st_row%s_use_risk%s_use_SepChunk%s_concat%s.csv\" % (ntree,start_row,use_risk_adjusted,use_separated_chunk,concat_results) \n",
    "    test_results.to_csv(save_test_df, index = False)\n",
    "else:\n",
    "    # removing the code to train on non random generated data as it did not work as discussed in project\n",
    "    pass\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This code is to train the SVM model on randomly generated data.\n",
    "'''\n",
    "##### FOR PCA features  && train on random data!####\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from model_functions import *\n",
    "from run_decision_tree import *\n",
    "import datetime\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "# if you want to test on the withheld data then use the following file BLIND_STFEATURES.csv\n",
    "file_location = r\"ccyData.csv\"\n",
    "performance_store = {\"data_size\" : [], \"Accuracy_Score\" : [], \"ntree\": [],\n",
    "                     \"Info_Ratio\" : [], \"run_time\" : [], \"train_date_st\": [], \"test_date_st\": []}\n",
    "params_dict = set_params_random_forests()\n",
    "svm_dict = set_params_svm()\n",
    "use_random_train_data = params_dict['use_random_train_data']\n",
    "########################### Set Model Paramaters #############################\n",
    "# this looks back over a set period as the memory for the LSTM\n",
    "kernel = svm_dict['kernel'] \n",
    "'''\n",
    "--- If you want to loop through the costs then un comment the snippet below to make it a list of costs\n",
    "'''\n",
    "costs = svm_dict['cost']# [i for i in range(25,301,25)] # [21, 66]\n",
    "# if running pca, max features can only be same or less than the full total of features\n",
    "test_buffer = params_dict['test_buffer']\n",
    "data_size = params_dict['data_size'] #  initially using 1500 training points\n",
    "# I.e. append the data into one df over each training window, but also use all available up until that point\n",
    "concat_results = params_dict['concat_results']\n",
    "# if the number is > 1, then the code takes that as the number of test points you want to use\n",
    "test_split = params_dict['test_split'] # roughly one month test ahead, which is a one month retrain period\n",
    "# signal threshold, when using classifier\n",
    "thold = params_dict['thold']\n",
    "total_data_needed = get_total_data_needed(test_split,data_size,test_buffer)\n",
    "# standardisation window\n",
    "window = params_dict['window']\n",
    "###### Set Targets ##############\n",
    "trade_horizon = params_dict['trade_horizon'] # in hours\n",
    "use_risk_adjusted = params_dict['use_risk_adjusted'] # if True: training on the sharpe return else raw\n",
    "use_binary = params_dict['use_binary'] # set to true if you are using the risk adjusted and want it binary for classification score\n",
    "use_classifier = params_dict['use_classifier']\n",
    "use_pca = params_dict['use_pca'] # if = 0 then implies do not use pca in the model\n",
    "use_separated_chunk = params_dict['use_separated_chunk']\n",
    "################### Standardise Entire Dataset using rolling lookback windows ###############\n",
    "data_normed, model_features, features_to_standardise = initialise_process(file_location, trade_horizon, \n",
    "                                                 window, use_risk_adjusted, use_pca,use_random_train_data)\n",
    "#data_normed = data_normed.replace(np.nan, 0)\n",
    "start_row = data_size\n",
    " # Use a rolling window to train and test\n",
    "################ Loop through the full dataset in terms of the training and testing.\n",
    "if use_random_train_data:\n",
    "    random_data_location = r\"CcyRandomTrend.csv\"\n",
    "    train, model_features, features_to_standardise = initialise_process(random_data_location, trade_horizon, \n",
    "                                                 window, use_risk_adjusted, use_pca, use_random_train_data)\n",
    "    test, model_features, features_to_standardise  = initialise_process(file_location, trade_horizon, \n",
    "                                                 window, use_risk_adjusted, use_pca, \n",
    "                                                use_random_train_data= False)\n",
    "    if use_pca > 0:\n",
    "        train, test, var_explained = get_pca_features(train,test, features_to_standardise, use_pca)\n",
    "    train_sample = train[model_features]\n",
    "    test_sample = test[model_features]\n",
    "    for cost in costs:\n",
    "        results, acc_score = run_svm_model(train_sample, test_sample,use_classifier, use_risk_adjusted,kernel,cost)\n",
    "        test_results = backtester(results, test, trade_horizon)[0]\n",
    "        strat_return = backtester(results, test, trade_horizon)[1]\n",
    "        information_ratio = backtester(results, test, trade_horizon)[2]\n",
    "        train_date = 1 # train['Date'].iloc[0]\n",
    "        test_date = test['Date'].iloc[0]\n",
    "        run_time = \"random\"\n",
    "        performance_df = update_performance(data_size, cost, acc_score , information_ratio, \n",
    "                                            run_time, train_date, test_date, performance_store)\n",
    "    save_test_df = r\"randomData%s_COST%s_use_risk%s_use_SepChunk%s_concat%s_TH%s.csv\" % (kernel,\n",
    "                                    cost,use_risk_adjusted,use_separated_chunk,concat_results, trade_horizon) \n",
    "    test_results.to_csv(save_test_df, index = False)\n",
    "else:\n",
    "    # the training on real world data is not needed due to poor performance as stated previously.\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Recurrent neural Network Code\n",
    "NB. This must be run on 64mb Python and with Keras/Tensorflow installed\n",
    "'''\n",
    "##### FOR PCA features  && train on random data!####\n",
    "# This file is a simple implementation of the \n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ml_trading_module import *\n",
    "from model_functions import *\n",
    "import datetime\n",
    "import sys\n",
    "\n",
    "'''\n",
    "---- If running the withheld data then use this name ccyDataBLIND_LongTermFEATURES\n",
    "'''\n",
    "file_location = r\"ccyData.csv\"\n",
    "performance_store = {\"data_size\" : [], \"Accuracy_Score\" : [], \"epochs\": [],\n",
    "                     \"Info_Ratio\" : [], \"run_time\" : [], \"train_date_st\": [], \"test_date_st\": []}\n",
    "params_dict = set_params_random_forests()\n",
    "lstm_dict = set_params_LSTM()\n",
    "use_random_train_data = params_dict['use_random_train_data']\n",
    "########################### Set Model Paramaters #############################\n",
    "# this looks back over a set period as the memory for the LSTM\n",
    "EPOCH = lstm_dict['EPOCH']\n",
    "first_layer = lstm_dict['first_layer']\n",
    "second_layer = lstm_dict['second_layer']\n",
    "look_back = lstm_dict['look_back'] \n",
    "\n",
    "# [i for i in range(25,301,25)] # [21, 66]\n",
    "# if running pca, max features can only be same or less than the full total of features\n",
    "test_buffer = params_dict['test_buffer']\n",
    "data_size = params_dict['data_size'] #  initially using 1500 training points\n",
    "# I.e. append the data into one df over each training window, but also use all available up until that point\n",
    "concat_results = params_dict['concat_results']\n",
    "# if the number is > 1, then the code takes that as the number of test points you want to use\n",
    "test_split = params_dict['test_split'] # roughly one month test ahead, which is a one month retrain period\n",
    "# signal threshold, when using classifier\n",
    "thold = params_dict['thold']\n",
    "total_data_needed = get_total_data_needed(test_split,data_size,test_buffer)\n",
    "# standardisation window\n",
    "window = params_dict['window']\n",
    "###### Set Targets ##############\n",
    "trade_horizon = params_dict['trade_horizon'] # in hours\n",
    "use_risk_adjusted = params_dict['use_risk_adjusted'] # if True: training on the sharpe return else raw\n",
    "use_binary = params_dict['use_binary'] # set to true if you are using the risk adjusted and want it binary for classification score\n",
    "use_classifier = params_dict['use_classifier']\n",
    "use_pca = params_dict['use_pca'] # if = 0 then implies do not use pca in the model\n",
    "use_separated_chunk = params_dict['use_separated_chunk']\n",
    "################### Standardise Entire Dataset using rolling lookback windows ###############\n",
    "data_normed, model_features, features_to_standardise = initialise_process(file_location, trade_horizon, \n",
    "                                                 window, use_risk_adjusted, use_pca,use_random_train_data, use_binary)\n",
    "#data_normed = data_normed.replace(np.nan, 0)\n",
    "start_row = data_size\n",
    " # Use a rolling window to train and test\n",
    "################ Loop through the full dataset in terms of the training and testing.\n",
    "if use_random_train_data:\n",
    "    print(\"random starting, EPOCH %s\" % str(EPOCH))\n",
    "    random_data_location = r\"/storage/CcyRandomTrendLSTM.csv\"\n",
    "    train, model_features, features_to_standardise = initialise_process(random_data_location, trade_horizon, \n",
    "                                                 window, use_risk_adjusted, use_pca, use_binary,use_random_train_data)\n",
    "    test, model_features, features_to_standardise  = initialise_process(file_location, trade_horizon, \n",
    "                                                 window, use_risk_adjusted, use_pca, \n",
    "                                                 use_binary ,use_random_train_data = False)\n",
    "    if test.shape[0] <= (look_back+test_buffer+trade_horizon):\n",
    "        sys.exit()\n",
    "    if use_pca > 0:\n",
    "        train, test, var_explained = get_pca_features(train,test, features_to_standardise, use_pca)\n",
    "    train_sample = train[model_features].values\n",
    "    test_sample = test[model_features].head(test.shape[0]-39).values # had to do this to make it work\n",
    "\n",
    "     # Parse the values into the LSTM format\n",
    "    train_data , train_target, null_dates = create_dataset(train_sample,False, look_back, test)\n",
    "    test_data, test_target, target_dates = create_dataset(test_sample, True, look_back, test)\n",
    "    \n",
    "    # reshape seems to add another list around every observation\n",
    "    train_data = train_data.reshape(train_data.shape[0], look_back, train_data.shape[2])\n",
    "    train_target = train_target.reshape(train_target.shape[0], 1)\n",
    "    test_data = test_data.reshape(test_data.shape[0], look_back, test_data.shape[2])\n",
    "    test_target = test_target.reshape(test_target.shape[0], 1)\n",
    "    #### Set up model parameters\n",
    "    # Build up the model\n",
    "    BATCH_SIZE = None # previous was 32 batch and 33 lookback\n",
    "    no_features = train_data.shape[2]\n",
    "    model = Sequential()\n",
    "    #model.add(LSTM(first_layer,batch_input_shape = (BATCH_SIZE,look_back,no_features), return_sequences = True))\n",
    "    #model.add(LSTM(second_layer, return_sequences = False, activation=\"softmax\"))\n",
    "    #model.compile(loss = \"mean_absolute_error\", optimizer=\"adam\", metrics = ['accuracy'])\n",
    "    # newer method of learning\n",
    "    model.add(LSTM(first_layer, batch_input_shape = (BATCH_SIZE,look_back,no_features), return_sequences = True))\n",
    "    model.add(LSTM(second_layer, return_sequences = False, activation=\"softmax\"))\n",
    "    model.add(Dense(8, activation = \"tanh\"))\n",
    "    model.add(Dense(1, activation = \"linear\"))\n",
    "    model.compile(loss = \"mse\", optimizer=\"adam\", metrics = ['accuracy'])\n",
    "    # train the model\n",
    "    # verbose = 1 gives the output of the training.\n",
    "    start_time = datetime.datetime.now()\n",
    "    lstm_engine = model.fit(train_data,train_target,epochs = EPOCH,validation_data=(test_data,test_target), verbose= 1)\n",
    "    #lstm_engine = model.fit(train_data,train_target,epochs = EPOCH, validation_data=(test_data,test_target),batch_size = BATCH_SIZE, verbose = 1)\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig , ax = plt.subplots(1,1,figsize=(10,8))\n",
    "    plt.plot(lstm_engine.history['loss'])\n",
    "    plt.show()\n",
    "    run_time = datetime.datetime.now() - start_time\n",
    "    # run training on the test data\n",
    "    results = model.predict(test_data, batch_size = BATCH_SIZE, verbose = 1)\n",
    "    # The % threshold needed to trigger a signal either way\n",
    "    predicted = [i[0] for i in results] # [np.sign(i) for i in results] #[signal(i, thold) for i in results]\n",
    "    acc_score = get_accuracy([np.sign(i) for i in results], test_target)\n",
    "    # This needs to change to handle the change in the target\n",
    "    predictions = pd.DataFrame({\"Date\" : target_dates,\"Predictions\": predicted})\n",
    "    test_results = pd.merge(test,predictions,how=\"left\", on=\"Date\").fillna(0)\n",
    "    # calculate the returns of the signal\n",
    "    test_results[\"scaled_signal\"] = test_results['Predictions'].shift(2).rolling(trade_horizon).sum()/trade_horizon\n",
    "    # no shift needed as we have already done that in previous step\n",
    "    test_results['strat_returns'] = test_results['logret']*test_results['scaled_signal']\n",
    "    test_results['strat_returns_sum'] = test_results['strat_returns'].cumsum()\n",
    "    strat_return = test_results['strat_returns'].sum()\n",
    "    information_ratio = (test_results['strat_returns'].mean()*260)/(test_results['strat_returns'].std()*np.sqrt(260))\n",
    "    \n",
    "    # Store the data as needed\n",
    "    performance_store['data_size'].append(data_size)\n",
    "    performance_store['epochs'].append(EPOCH)\n",
    "    performance_store['Accuracy_Score'].append(acc_score)\n",
    "    performance_store['Info_Ratio'].append(information_ratio)\n",
    "    performance_store['run_time'].append(run_time)\n",
    "    performance_store['train_date_st'].append(test_results['Date'].iloc[0])\n",
    "    performance_store['test_date_st'].append(test_results['Date'].iloc[-1])\n",
    "    performance_df = pd.DataFrame(performance_store)\n",
    "    save_results = r\"/storage/test_result_start_row%s_lkbk%s_Epochs%s_thold%s.csv\" % (start_row, look_back, EPOCH, thold)\n",
    "    test_results.to_csv(save_results,index = False)\n",
    "    performance_df.to_csv(r\"/storage/performance_df_start_row%s_lkbk%s_Epochs%s_thold%s.csv\" % (start_row, look_back, EPOCH, thold))\n",
    "else:\n",
    "    # if not using random data then move to the normal method.\n",
    "    ################ Loop through the full dataset in terms of the training and testing.\n",
    "    while start_row < data_normed.shape[0]:\n",
    "        # first check if there is enough data left\n",
    "        if (start_row + total_data_needed) > data_normed.shape[0]:\n",
    "            # if we are about to go over the limit, then just return the last data_size + test size proportion of data\n",
    "            trunc_data = data_normed.iloc[-total_data_needed:,:]\n",
    "        # we need to increment over the data size\n",
    "        if use_separated_chunk:\n",
    "            # this means we jump across the full previous train and test data\n",
    "            trunc_data = data_normed.loc[start_row:,:]\n",
    "            start_row += total_data_needed\n",
    "        if concat_results:\n",
    "            # in this instance, we can to add to the start row first before chunking the data\n",
    "            start_row += test_split\n",
    "            # we are training on all data available up until that point, and testing x timeperiods ahead\n",
    "            trunc_data = data_normed.loc[:start_row,:]  \n",
    "        else:\n",
    "            # this rolls the data so that the new training will overlap on the old test set and create a new separated test set\n",
    "            trunc_data = data_normed.loc[start_row:,:]\n",
    "            start_row += data_size\n",
    "        # standardise the data\n",
    "        #################### Set up training and testing ########################\n",
    "        \n",
    "        # create data_set\n",
    "        train , test = create_train_test_file(trunc_data, data_size, test_split, test_buffer)\n",
    "        if test.shape[0] <= (look_back+test_buffer+trade_horizon):\n",
    "            break\n",
    "        if use_pca > 0:\n",
    "            train, test, var_explained = get_pca_features(train,test, features_to_standardise, use_pca)\n",
    "        train_sample = train[model_features].values\n",
    "        test_sample = test[model_features].values\n",
    "    \n",
    "         # Parse the values into the LSTM format\n",
    "        train_data , train_target, null_dates = create_dataset(train_sample,False, look_back, test)\n",
    "        test_data, test_target, target_dates = create_dataset(test_sample, True, look_back, test)\n",
    "        \n",
    "        # reshape seems to add another list around every observation\n",
    "        train_data = train_data.reshape(train_data.shape[0], look_back, train_data.shape[2])\n",
    "        train_target = train_target.reshape(train_target.shape[0], 1)\n",
    "        test_data = test_data.reshape(test_data.shape[0], look_back, test_data.shape[2])\n",
    "        test_target = test_target.reshape(test_target.shape[0], 1)\n",
    "        #### Set up model parameters\n",
    "        # Build up the model\n",
    "        BATCH_SIZE = 6\n",
    "        no_features = train_data.shape[2]\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(120,batch_input_shape = (None,look_back,no_features), return_sequences = True))\n",
    "        model.add(LSTM(1, return_sequences = False, activation=\"softmax\"))\n",
    "        model.compile(loss = \"mean_absolute_error\", optimizer=\"adam\", metrics = ['accuracy'])\n",
    "        \n",
    "        EPOCH = 350\n",
    "        # train the model\n",
    "        # verbose = 1 gives the output of the training.\n",
    "        start_time = datetime.datetime.now()\n",
    "        lstm_engine = model.fit(train_data,train_target,epochs = EPOCH,validation_data=(test_data,test_target), verbose= 1)\n",
    "        run_time = datetime.datetime.now() - start_time\n",
    "        # run training on the test data\n",
    "        results = model.predict(test_data)\n",
    "        # The % threshold needed to trigger a signal either way\n",
    "        #thold = 0.55\n",
    "        predicted = [np.sign(i) for i in results] # [signal(i, thold) for i in results]\n",
    "        acc_score = get_accuracy(predicted, test_target)\n",
    "        # This needs to change to handle the change in the target\n",
    "        predictions = pd.DataFrame({\"Date\" : target_dates,\"Predictions\": predicted})\n",
    "        test_results = pd.merge(test,predictions,how=\"left\", on=\"Date\").fillna(0)\n",
    "        # calculate the returns of the signal\n",
    "        test_results[\"scaled_signal\"] = test_results['Predictions'].shift(2).rolling(trade_horizon).sum()/trade_horizon\n",
    "        # no shift needed as we have already done that in previous step\n",
    "        test_results['strat_returns'] = test_results['logret']*test_results['scaled_signal']\n",
    "        test_results['strat_returns_sum'] = test_results['strat_returns'].cumsum()\n",
    "        strat_return = test_results['strat_returns'].sum()\n",
    "        information_ratio = (test_results['strat_returns'].mean()*260)/(test_results['strat_returns'].std()*np.sqrt(260))\n",
    "        \n",
    "        # Store the data as needed\n",
    "        performance_store['data_size'].append(data_size)\n",
    "        performance_store['epochs'].append(EPOCH)\n",
    "        performance_store['Accuracy_Score'].append(acc_score)\n",
    "        performance_store['Info_Ratio'].append(information_ratio)\n",
    "        performance_store['run_time'].append(run_time)\n",
    "        performance_store['train_date_st'].append(test_results['Date'].iloc[0])\n",
    "        performance_store['test_date_st'].append(test_results['Date'].iloc[-1])\n",
    "        performance_df = pd.DataFrame(performance_store)\n",
    "        save_results = r\"/storage/test_result_start_row%s_lkbk%s_Epochs%s_thold%s.csv\" % (start_row, look_back, EPOCH, thold)\n",
    "        test_results.to_csv(save_results,index = False)\n",
    "        performance_df.to_csv(r\"/storage/performance_df_start_row%s_lkbk%s_Epochs%s_thold%s.csv\" % (start_row, look_back, EPOCH, thold))\n",
    "\n",
    "'''\n",
    "Code for plotting the accuracy metrics\n",
    "'''\n",
    "# quick testing of what is happening with this LSTM\n",
    "import matplotlib.pyplot as plt\n",
    "#fig , ax = plt.subplots(1,1,figsize=(12,8))\n",
    "# lets see the history of the error update.\n",
    "# want a smooth learning line\n",
    "#ax.plot(lstm_engine.history['loss'])\n",
    "import matplotlib.pyplot as plt\n",
    "fig , ax = plt.subplots(1,1,figsize=(10,8))\n",
    "plt.plot(lstm_engine.history['loss'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This code performs the model 2 trend estimation, where we take in te return of the trend model in each case of the SVM, RF and LSTM\n",
    "\n",
    "1. We get the features of the macro economic factors\n",
    "'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from create_model_features import trends_features\n",
    "from copy import deepcopy\n",
    "import datetime\n",
    "################## INSERT CCY You want to RUN\n",
    "ccy = \"EURUSD\"\n",
    "ccy1 = ccy[:3]\n",
    "ccy2 = ccy[3:]\n",
    "edi_location = r\"EDI.csv\"\n",
    "esi_location = r\"ESI.csv\"\n",
    "features = [\"EDI_\"+ccy1 + ccy2 + \"_spread\", \"EDI_G10_normalised\", \"EDI_Global_normalised\",\n",
    "            \"ESI_\"+ccy1 + ccy2 + \"_spread\", \"ESI_G10_normalised\", \"ESI_Global_normalised\"]\n",
    "EDI = pd.read_csv(edi_location)\n",
    "ESI = pd.read_csv(esi_location)\n",
    "EDI['Date'] =  pd.to_datetime(EDI['Date'], format= '%d/%m/%Y')\n",
    "ESI['Date'] =  pd.to_datetime(ESI['Date'], format= '%d/%m/%Y')\n",
    "EDI['Date'] =  EDI['Date'].dt.date\n",
    "ESI['Date'] =  ESI['Date'].dt.date\n",
    "EDI[\"EDI_\"+ ccy1 + ccy2 + \"_spread\"] = EDI[\"EDI_\" +ccy1 + \"_normalised\"] - EDI[\"EDI_\" +ccy2 + \"_normalised\"]\n",
    "ESI[\"ESI_\"+ ccy1 + ccy2 + \"_spread\"] = ESI[\"ESI_\" +ccy1 + \"_normalised\"] - ESI[\"ESI_\" + ccy2 + \"_normalised\"]\n",
    "EDI_tradable = EDI[[\"EDI_\"+ccy1 + ccy2 + \"_spread\", \"EDI_G10_normalised\", \"EDI_Global_normalised\"]].cumsum()\n",
    "ESI_tradable = ESI[[\"ESI_\"+ccy1 + ccy2 + \"_spread\", \"ESI_G10_normalised\", \"ESI_Global_normalised\"]].cumsum()\n",
    "EDI_tradable['Date'] = EDI['Date']\n",
    "ESI_tradable['Date'] = ESI['Date']\n",
    "short, medium, long, longest, medium_multiplier,long_multplier = 21, 55, 100, 200, 1, 1 # as all in days.\n",
    "for col in features:\n",
    "    if col in list(EDI_tradable.columns):\n",
    "        data = deepcopy(EDI_tradable)\n",
    "        data = trends_features(data,col, short, medium, long,longest,medium_multiplier,long_multplier)\n",
    "        EDI_tradable[col+\"_spotvma\"] = data['spot_v_HF']\n",
    "        EDI_tradable[col+\"_madiff\"] = data['HF_ema_diff']\n",
    "    if col in list(ESI_tradable.columns):\n",
    "        data = deepcopy(ESI_tradable)\n",
    "        data = trends_features(data,col, short, medium, long, longest,medium_multiplier,long_multplier)\n",
    "        ESI_tradable[col+\"_spotvma\"] = data['spot_v_HF']\n",
    "        ESI_tradable[col+\"_madiff\"] = data['HF_ema_diff']\n",
    "print(EDI_tradable.columns)\n",
    "'''\n",
    "------ Save file name as Signals_FullsamplewBlind.csv if running on the with held data----\n",
    "'''\n",
    "signal_location = r\"Signals_Fullsample.csv\"\n",
    "signals = pd.read_csv(signal_location)\n",
    "signals['Date'] = pd.to_datetime(signals['Date'], format= '%d/%m/%Y %H:%M')\n",
    "signals['DDMMYY'] = signals['Date'].dt.date # signals['Date'].apply(get_ddmmyy)\n",
    "#signals['DDMMYY'] = pd.to_datetime(signals['DDMMYY'])\n",
    "trend_estimation = signals.groupby('DDMMYY').sum().reset_index(drop = False)\n",
    "# i.e. the average of the ccy kevel across each hour\n",
    "trend_estimation['CCY'] = signals.groupby('DDMMYY').mean().reset_index(drop = False)['CCY']\n",
    "# overwriting the dte arg as its easier to understand\n",
    "trend_estimation['Date'] = trend_estimation['DDMMYY']\n",
    "trend_estimation[ccy1+ccy2 +\"_vol\"] = trend_estimation['logret'].rolling(60).std()*np.sqrt(260)\n",
    "trend_estimation = pd.merge(trend_estimation,EDI_tradable,how=\"left\" ,on= \"Date\")\n",
    "trend_estimation = pd.merge(trend_estimation,ESI_tradable,how=\"left\" ,on= \"Date\")\n",
    "'''\n",
    "----- Save te file as trendestimationwBlind.csv if running with with held data\n",
    "'''\n",
    "trend_location = r\"trendestimation.csv\"\n",
    "trend_estimation.replace(np.nan,0).to_csv(trend_location)\n",
    "\n",
    "from model_functions import standardise_data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from create_model_features import trends_features\n",
    "from copy import deepcopy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from model_functions import create_train_test_file\n",
    "from run_decision_tree import set_params_trend_estimate, set_params_random_forests\n",
    "rf_dict = set_params_random_forests()\n",
    "trend_estimate_dict = set_params_trend_estimate()\n",
    "trade_horizon = trend_estimate_dict['trade_horizon'] # roughly 1 month ahead.\n",
    "std_window = trend_estimate_dict['std_window'] # 1 year\n",
    "train_size = trend_estimate_dict['train_size']# no. data points in the train\n",
    "test_split = trend_estimate_dict['test_split']\n",
    "test_buffer = trend_estimate_dict['test_buffer']\n",
    "concat_results = False\n",
    "# open trend estimation file from here\n",
    "trend_location = r\"trendestimation.csv\"\n",
    "trend_estimation = pd.read_csv(trend_location)\n",
    "# list of available strats\n",
    "trend_strats = [\"Linear\",\"SVM_erf\",\"RF_erf\",\"LSTM1_erf\",\"LSTM2_erf\"]\n",
    "trend_strats_blind = [\"SVM\",\"RF\", \"LSTM\"]\n",
    "# trend features\n",
    "T_features = [\"EURUSD_vol\",\"ESI_EURUSD_spread_spotvma\", \"ESI_EURUSD_spread_madiff\",       \"ESI_G10_normalised_spotvma\",\n",
    "                   \"ESI_G10_normalised_madiff\", \"ESI_Global_normalised_spotvma\", \"ESI_Global_normalised_madiff\",\n",
    "                   \"EDI_EURUSD_spread_spotvma\", \"EDI_EURUSD_spread_madiff\",       \"EDI_G10_normalised_spotvma\",\n",
    "                   \"EDI_G10_normalised_madiff\", \"EDI_Global_normalised_spotvma\",  \"EDI_Global_normalised_madiff\"]\n",
    "dynamic_T_features = [\"ESI_Global_normalised_madiff\", \"EDI_EURUSD_spread_spotvma\", \"EDI_G10_normalised_spotvma\", \n",
    "                      \"EDI_Global_normalised_madiff\"]\n",
    "feats_to_use = T_features  # can place t feats here too if needed\n",
    "'''\n",
    "------ Change the below to any one of these [\"Linear\",\"SVM_erf\",\"RF_erf\",\"LSTM1_erf\",\"LSTM2_erf\"] models\n",
    "'''\n",
    "trend_model = \"RF_erf\"\n",
    "feats = deepcopy(feats_to_use)\n",
    "feats.append(\"Date\")\n",
    "feats.append(trend_model)\n",
    "# now kick off the learning approach\n",
    "df = trend_estimation[feats]\n",
    "df['target'] = df[trend_model].iloc[::-1].shift(2).rolling(trade_horizon).sum().values[::-1]\n",
    "# maybe best to standardise on the sklearn methods\n",
    "# df = standardise_data(df, feats, T_features , std_window)\n",
    "data_size = int(df.shape[0]*train_size) # data size implies the size of the training set\n",
    "train, test = create_train_test_file(df, data_size, test_split, test_buffer, concat_results)\n",
    "\n",
    "'''\n",
    "---- NOw run the random forest engine on this code\n",
    "'''\n",
    "from sklearn import tree\n",
    "from run_decision_tree import decision_tree\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "scaler = StandardScaler() \n",
    "use_RF = rf_dict['use_RF']\n",
    "train_sample = pd.DataFrame(scaler.fit_transform(train[feats_to_use]), columns = list(feats_to_use))\n",
    "test_sample = pd.DataFrame(scaler.transform(test[feats_to_use]), columns = list(feats_to_use))\n",
    "use_classifier = rf_dict['use_classifier']\n",
    "use_risk_adjusted = rf_dict['use_risk_adjusted']\n",
    "ntree = rf_dict['ntrees'][0]\n",
    "max_features = rf_dict['max_features']\n",
    "max_depth = rf_dict['max_depth']\n",
    "# if you want to run a random forest.\n",
    "if use_RF:\n",
    "    train_sample['target'] = train['target']\n",
    "    test_sample['target'] = test['target']\n",
    "    results, acc_score = decision_tree(train_sample, test_sample,use_classifier, use_risk_adjusted,\n",
    "                                       ntree, max_features, max_depth)\n",
    "if use_classifier:\n",
    "    train_sample['target'] = train['target'].apply(np.sign)\n",
    "    test_sample['target'] = test['target'].apply(np.sign)\n",
    "    clf = tree.DecisionTreeClassifier(max_leaf_nodes = 6, max_depth = 12)\n",
    "    RF = RandomForestClassifier(n_estimators=ntree, max_features= max_features,max_depth = max_depth, verbose=0)\n",
    "else:\n",
    "    clf = tree.DecisionTreeRegressor(max_leaf_nodes = 6, max_depth = 12)\n",
    "    RF = RandomForestRegressor(n_estimators=ntree, max_features= max_features, max_depth = max_depth,verbose=0)\n",
    "#clf_model = clf.fit(train_sample[[\"EDI_EURUSD_spread_spotvma\",\"EDI_G10_normalised_spotvma\"]],train_sample['target'])\n",
    "rf_model = RF.fit(train_sample.iloc[:,:-1], train_sample['target'])\n",
    "\n",
    "'''\n",
    "-----Create the variable Importance plot\n",
    "'''\n",
    "from matplotlib import pyplot as plt\n",
    "importances = rf_model.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "plt.figure(figsize=(30,14))\n",
    "plt.title('Relative Feature Importances', fontsize=20)\n",
    "plt.barh(range(len(importances)), importances, color='b', align='center')\n",
    "#plt.yticks(range(len(indices)), )\n",
    "plt.xlabel('Relative Importance Score', fontsize=20)\n",
    "plt.yticks(range(len(importances)), T_features, fontsize=18)\n",
    "varimp_location = r\"Varimp_%s.png\" % (trend_model)\n",
    "plt.savefig(varimp_location)\n",
    "#plt.show()\n",
    "'''\n",
    "---- Now calculate the model returns\n",
    "'''\n",
    "def get_trade(row):\n",
    "    '''\n",
    "    \n",
    "    :param row: \n",
    "    :return: \n",
    "    '''\n",
    "    if row > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def trend_estimate_backtester(test, results, trend_model):\n",
    "    predictions = pd.DataFrame({\"Date\": test['Date'], \"Predictions\": results})\n",
    "    test_results = pd.merge(test, predictions, how=\"left\", on=\"Date\").fillna(0)\n",
    "    # calculate the returns of the signal\n",
    "    test_results[\"scaled_signal\"] = test_results['Predictions'].shift(2).rolling(trade_horizon).sum() / trade_horizon\n",
    "    test_results['trend_buy_sell'] = test_results['scaled_signal'].apply(get_trade)\n",
    "    test_results['strat_returns'] = test_results[trend_model] * test_results['trend_buy_sell']\n",
    "    test_results['strat_returns_sum'] = test_results['strat_returns'].cumsum()\n",
    "    test_results['nonfiltered_ret'] = test_results[trend_model].cumsum()\n",
    "    return test_results\n",
    "test_results = trend_estimate_backtester(test,results,trend_model)\n",
    "# if saving the withheld data then save file as BLIND_csv\n",
    "test_location = r\"model%s_TH%s_ntree%s_classifier%s_riskadj%s.csv\" % (\n",
    "    trend_model, trade_horizon, ntree ,use_classifier, use_risk_adjusted)\n",
    "test_results.to_csv(test_location, index = False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This code creates the process for the random data generation\n",
    "'''\n",
    "\n",
    "'''\n",
    "This file generates the trenging price series on which the model will train on.\n",
    "'''\n",
    "from matplotlib import pyplot as plt\n",
    "from generate_trend import get_trendy_data\n",
    "from create_model_features import trends_features \n",
    "import numpy as np\n",
    "file_location = r\"CcyRandomTrend.csv\"\n",
    "n_samples = 5000 # for 50000 if training the SVM and RF\n",
    "trend_strength = 0.068 # alpha a value between -0.5 to 0.15\n",
    "pct_stdev = 0.008 # stdev value between 0.001 and 0.01\n",
    "short = 5\n",
    "medium = 21\n",
    "long = 55\n",
    "longest = 100\n",
    "medium_multiplier = 24 # one day in hours\n",
    "long_multplier = 120 # week in hours\n",
    "CCY_COL = \"trend\"\n",
    "ccy_data = get_trendy_data(n_samples,trend_strength,pct_stdev,CCY_COL,\n",
    "                           short,medium,long,longest,medium_multiplier,long_multplier)\n",
    "ccy_data.to_csv(file_location)\n",
    "ccy_data['CCY'].plot()\n",
    "ccy_data['LF_short'].plot()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
